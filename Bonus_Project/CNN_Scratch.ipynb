{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 22:13:54.991831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer:\n",
    "    def __init__(self, kernel_size, number_of_kernels, input_shape):\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\" \n",
    "        self.input_channels = input_shape[2]\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.number_of_kernels = number_of_kernels\n",
    "        self.kernels = np.random.randn(kernel_size, kernel_size, self.input_channels, number_of_kernels)\n",
    "        \n",
    "        # output shape\n",
    "        self.output_shape = (input_shape[0] - kernel_size + 1, input_shape[1] - kernel_size + 1, number_of_kernels)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. \" + \"Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "\n",
    "        output = np.zeros(self.output_shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                output[:, :, i] += signal.correlate2d(input[:, :, j], self.kernels[:, :, j, i], mode='valid')\n",
    "        \n",
    "        output += self.biases\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size) + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size) + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        \n",
    "        # Compute gradient with respect to kernels\n",
    "        kernel_grads = np.zeros(self.kernels.shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                kernel_grads[:, :, j, i] = signal.correlate2d(input[:, :, j], gradWRTMyOutput[:, :, i], mode='valid')\n",
    "        \n",
    "        # Compute gradient with respect to biases\n",
    "        bias_grads = gradWRTMyOutput.copy()\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        for j in range(self.input_channels):\n",
    "            for i in range(self.number_of_kernels):\n",
    "                input_grads[:, :, j] += signal.convolve2d(gradWRTMyOutput[:, :, i], self.kernels[:, :, j, i], mode='full')\n",
    "        \n",
    "        # Update kernels and biases\n",
    "        self.kernels -= learning_rate * kernel_grads\n",
    "        self.biases -= learning_rate * bias_grads\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z')\n",
    "\n",
    "Error_Grad_WRT_Input: Error_Grad_WRT_Output * Output_Grad_WRT_Input , where * is element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = 1.0 / (1.0 + np.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at sigmoid layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at sigmoid layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        # element-wise gradient\n",
    "        gradMyOutputWRTMyInput = self.sigmoid_derivative(self.output)\n",
    "        # element-wise multiplication\n",
    "        return gradWRTMyOutput * gradMyOutputWRTMyInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Pooling Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z)\n",
    "\n",
    "Error Gradient\n",
    "\n",
    "$\\frac{dE}{dX_{ijk}} = \\sum_{m,n} \\frac{dE}{dY_{mnk}} \\times \\frac{1}{\\text{pool\\_size}^2} \\times \\mathbb{\\theta}_{i, j}$ where $\\mathbb{\\theta}_{i, j}$ is 1 if $X_{ijk}$ was included when calculating $Y_{mnk}$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePoolLayer:\n",
    "    def __init__(self, input_shape, pool_size, stride):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        # input shape must be divisible by pool size\n",
    "        assert input_shape[0] % pool_size == 0, \"Input shape must be divisible by pool size\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (input_shape[0] // stride, input_shape[1] // stride, input_shape[2])\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "        for i in range(self.output_shape[0]):\n",
    "            for j in range(self.output_shape[1]):\n",
    "                for k in range(self.output_shape[2]):\n",
    "                    self.output[i, j, k] = np.mean(input[i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size, k])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at average pool layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at average pool layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        avg_pool_n = self.pool_size * self.pool_size # number of elements involved in the average pool operation\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        \n",
    "        for channel in range(self.input_shape[2]):\n",
    "            for i in range(self.output_shape[0]):\n",
    "                for j in range(self.output_shape[1]):\n",
    "                    # Only loop over the elements involved in the average pool operation\n",
    "                    for x in range(i*self.stride, i*self.stride+self.pool_size):\n",
    "                        for y in range(j*self.stride, j*self.stride+self.pool_size):\n",
    "                            input_grads[x, y, channel] += gradWRTMyOutput[i, j, channel] / avg_pool_n\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer():\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (np.prod(input_shape),)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = input.flatten()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at flatten layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at flatten layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Layer\n",
    "Softmax Derivative: $\\frac{dS_i}{dX_j} = S_i(1 - S_i)$ if $i = j$ and $-S_iS_j$ otherwise\n",
    "\n",
    "In matrix form, obtain $C = softmax(x) \\times softmax(x)^T$ and $D = diag(softmax(x))$ where $D$ is a diagonal matrix with the softmax values on the diagonal and 0 elsewhere.\n",
    "\n",
    "$K = D - C$\n",
    "Then, $\\frac{dS}{dX} = K \\times \\frac{dE}{dO}$ where $O$ is the output of the softmax layer and $E$ is the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 1D\n",
    "        assert len(input_shape) == 1, \"Input shape must be 1D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = np.exp(input) / np.sum(np.exp(input))\n",
    "        return self.output\n",
    "    \n",
    "    # def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "    #     assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at softmax layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "    #     assert input.shape == self.input_shape, \"Input shape is not correct at softmax layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        \n",
    "    #     # Compute gradient with respect to input\n",
    "    #     input_grads = np.zeros(input.shape)\n",
    "    #     inp_softmax = self.softmax(input)\n",
    "    #     softmax_matrix = np.reshape(inp_softmax, (inp_softmax.shape[0], 1)) @ np.reshape(inp_softmax, (1, inp_softmax.shape[0]))\n",
    "    #     softmax_matrix = -1 * softmax_matrix\n",
    "    #     np.fill_diagonal(softmax_matrix, inp_softmax * (1 - inp_softmax))\n",
    "    #     input_grads = softmax_matrix @ gradWRTMyOutput\n",
    "        \n",
    "    #     return input_grads\n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape\n",
    "        assert input.shape == self.input_shape\n",
    "\n",
    "        # Compute the softmax for the input\n",
    "        inp_softmax = self.softmax(input)\n",
    "\n",
    "        # Create an empty Jacobian matrix\n",
    "        jacobian_matrix = np.zeros((len(inp_softmax), len(inp_softmax)))\n",
    "\n",
    "        # Fill in the Jacobian matrix\n",
    "        for i in range(len(inp_softmax)):\n",
    "            for j in range(len(inp_softmax)):\n",
    "                if i == j:\n",
    "                    jacobian_matrix[i][j] = inp_softmax[i] * (1 - inp_softmax[i])\n",
    "                else:\n",
    "                    jacobian_matrix[i][j] = -inp_softmax[i] * inp_softmax[j]\n",
    "\n",
    "        # Compute the gradient\n",
    "        input_grads = np.dot(jacobian_matrix, gradWRTMyOutput)\n",
    "\n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss Layer\n",
    "\n",
    "$\\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]$\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\text{Loss}}{\\partial p_i} = -\\frac{1}{N} \\left( \\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 1D, typically the output of a softmax layer for two classes\n",
    "        assert len(input_shape) == 1, \"Input shape must be 1D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (1,)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensuring input and target shapes are correct\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        assert target.shape == self.input_shape, \"Target shape is not correct. Expected: \" + str(self.input_shape) + \" Actual: \" + str(target.shape)\n",
    "\n",
    "        # Calculating cross entropy loss\n",
    "        # Adding a small value to prevent log(0)\n",
    "        eps = 1e-15\n",
    "        self.output = -np.mean(target * np.log(input + eps) + (1 - target) * np.log(1 - input + eps))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, target, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        # Ensuring input and target shapes are correct\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at loss layer. Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        assert target.shape == self.input_shape, \"Target shape is not correct at loss layer. Expected: \" + str(self.input_shape) + \" Actual: \" + str(target.shape)\n",
    "\n",
    "        # Calculating gradient\n",
    "        # Adding a small value to prevent division by zero\n",
    "        eps = 1e-15\n",
    "        return -(target / (input + eps) - (1 - target) / (1 - input + eps)) / input.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Layer (Helper Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer:\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. \" + \"Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = input.reshape(self.output_shape)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at reshape layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at reshape layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCorrect(prediction, target):\n",
    "    return np.argmax(prediction) == np.argmax(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Display first image\n",
    "Image.fromarray(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Reshape the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # (height, width, channel)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) # (height, width, channel)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(0)\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.9044925042331073\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = [\n",
    "    CNNLayer(kernel_size=3, number_of_kernels=2, input_shape=(28, 28, 1)),\n",
    "    SigmoidLayer(input_shape=(26, 26, 2)),\n",
    "    AveragePoolLayer(input_shape=(26, 26, 2), pool_size=2, stride=2),\n",
    "    FlattenLayer(input_shape=(13, 13, 2)),\n",
    "    ReshapeLayer(input_shape=(338,), output_shape=(1,1,338)),\n",
    "    CNNLayer(kernel_size=1, number_of_kernels=10, input_shape=(1,1,338)),\n",
    "    ReshapeLayer(input_shape=(1,1,10), output_shape=(10,)),\n",
    "    SoftmaxLayer(input_shape=(10,))\n",
    "]\n",
    "\n",
    "loss_layer = CrossEntropyLoss(input_shape=(10,))\n",
    "\n",
    "# Forward pass\n",
    "x = x_train[1]\n",
    "y = y_train[1]\n",
    "x = model[0].forward(x)\n",
    "x = model[1].forward(x)\n",
    "x = model[2].forward(x)\n",
    "x = model[3].forward(x)\n",
    "x = model[4].forward(x)\n",
    "x = model[5].forward(x)\n",
    "x = model[6].forward(x)\n",
    "x = model[7].forward(x)\n",
    "loss = loss_layer.forward(x, y)\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 2.588744263314928, Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 0.07601143574590724, Accuracy: 0.14\n",
      "Epoch: 0, Iteration: 200, Loss: 0.5564269202397553, Accuracy: 0.13\n",
      "Epoch: 0, Iteration: 300, Loss: 0.8250777199999411, Accuracy: 0.15\n",
      "Epoch: 0, Iteration: 400, Loss: 0.30033168054756704, Accuracy: 0.19\n",
      "Epoch: 0, Iteration: 500, Loss: 0.2655829394801745, Accuracy: 0.1\n",
      "Epoch: 0, Iteration: 600, Loss: 0.3069032833647564, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 700, Loss: 0.5411470470429335, Accuracy: 0.2\n",
      "Epoch: 0, Iteration: 800, Loss: 0.12997636776698035, Accuracy: 0.29\n",
      "Epoch: 0, Iteration: 900, Loss: 0.5426570530837228, Accuracy: 0.1\n",
      "Epoch: 0, Iteration: 1000, Loss: 0.26815550368887825, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 1100, Loss: 0.2648740111711352, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 1200, Loss: 0.12339795744686341, Accuracy: 0.27\n",
      "Epoch: 0, Iteration: 1300, Loss: 0.3772681138534869, Accuracy: 0.22\n",
      "Epoch: 0, Iteration: 1400, Loss: 0.06410675230751153, Accuracy: 0.2\n",
      "Epoch: 0, Iteration: 1500, Loss: 0.17511056785547724, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 1600, Loss: 0.29590052095143043, Accuracy: 0.21\n",
      "Epoch: 0, Iteration: 1700, Loss: 0.4867199266696189, Accuracy: 0.33\n",
      "Epoch: 0, Iteration: 1800, Loss: 0.5983066037757945, Accuracy: 0.3\n",
      "Epoch: 0, Iteration: 1900, Loss: 0.3009227696112952, Accuracy: 0.27\n",
      "Epoch: 0, Iteration: 2000, Loss: 0.024388583279080975, Accuracy: 0.33\n",
      "Epoch: 0, Iteration: 2100, Loss: 0.1436397601311616, Accuracy: 0.31\n",
      "Epoch: 0, Iteration: 2200, Loss: 0.08916829101679925, Accuracy: 0.45\n",
      "Epoch: 0, Iteration: 2300, Loss: 0.10949858022953836, Accuracy: 0.46\n",
      "Epoch: 0, Iteration: 2400, Loss: 0.29504737028125105, Accuracy: 0.43\n",
      "Epoch: 0, Iteration: 2500, Loss: 0.029426333179572384, Accuracy: 0.35\n",
      "Epoch: 0, Iteration: 2600, Loss: 0.14582994359314821, Accuracy: 0.39\n",
      "Epoch: 0, Iteration: 2700, Loss: 0.35521184141861617, Accuracy: 0.5\n",
      "Epoch: 0, Iteration: 2800, Loss: 0.41389604711425887, Accuracy: 0.41\n",
      "Epoch: 0, Iteration: 2900, Loss: 0.047030651146234, Accuracy: 0.51\n",
      "Epoch: 0, Iteration: 3000, Loss: 0.4023842421658436, Accuracy: 0.46\n",
      "Epoch: 0, Iteration: 3100, Loss: 0.05444547809185183, Accuracy: 0.5\n",
      "Epoch: 0, Iteration: 3200, Loss: 0.10206562710867453, Accuracy: 0.49\n",
      "Epoch: 0, Iteration: 3300, Loss: 0.5140524275740147, Accuracy: 0.53\n",
      "Epoch: 0, Iteration: 3400, Loss: 0.20813280948521767, Accuracy: 0.52\n",
      "Epoch: 0, Iteration: 3500, Loss: 0.2196527780817512, Accuracy: 0.49\n",
      "Epoch: 0, Iteration: 3600, Loss: 0.46140957440885827, Accuracy: 0.42\n",
      "Epoch: 0, Iteration: 3700, Loss: 0.15013380690060973, Accuracy: 0.55\n",
      "Epoch: 0, Iteration: 3800, Loss: 0.045001991227222246, Accuracy: 0.5\n",
      "Epoch: 0, Iteration: 3900, Loss: 0.0028830659792345144, Accuracy: 0.55\n",
      "Epoch: 0, Iteration: 4000, Loss: 0.12313644423264021, Accuracy: 0.62\n",
      "Epoch: 0, Iteration: 4100, Loss: 0.1242127754226893, Accuracy: 0.6\n",
      "Epoch: 0, Iteration: 4200, Loss: 0.5398319165769977, Accuracy: 0.55\n",
      "Epoch: 0, Iteration: 4300, Loss: 0.25283747670823253, Accuracy: 0.63\n",
      "Epoch: 0, Iteration: 4400, Loss: 0.16270593412913428, Accuracy: 0.57\n",
      "Epoch: 0, Iteration: 4500, Loss: 0.1319682645368723, Accuracy: 0.61\n",
      "Epoch: 0, Iteration: 4600, Loss: 0.014733879955084305, Accuracy: 0.58\n",
      "Epoch: 0, Iteration: 4700, Loss: 0.07245870687644568, Accuracy: 0.59\n",
      "Epoch: 0, Iteration: 4800, Loss: 0.010232971839168417, Accuracy: 0.62\n",
      "Epoch: 0, Iteration: 4900, Loss: 0.4549251391240035, Accuracy: 0.6\n",
      "Epoch: 0, Iteration: 5000, Loss: 0.307011619512354, Accuracy: 0.59\n",
      "Epoch: 0, Iteration: 5100, Loss: 0.13250003243040584, Accuracy: 0.63\n",
      "Epoch: 0, Iteration: 5200, Loss: 0.08412114369802554, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 5300, Loss: 0.01664546813943734, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 5400, Loss: 0.5415958168641438, Accuracy: 0.62\n",
      "Epoch: 0, Iteration: 5500, Loss: 0.027651314532594385, Accuracy: 0.68\n",
      "Epoch: 0, Iteration: 5600, Loss: 0.04162804986715408, Accuracy: 0.65\n",
      "Epoch: 0, Iteration: 5700, Loss: 0.1175339099384454, Accuracy: 0.68\n",
      "Epoch: 0, Iteration: 5800, Loss: 0.18215962296489419, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 5900, Loss: 0.2503621509815501, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 6000, Loss: 0.029762703096626875, Accuracy: 0.65\n",
      "Epoch: 0, Iteration: 6100, Loss: 0.16982923855234855, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 6200, Loss: 0.38342161481305914, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 6300, Loss: 0.1013886569131176, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 6400, Loss: 0.08950821144339363, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 6500, Loss: 0.15092839398102328, Accuracy: 0.72\n",
      "Epoch: 0, Iteration: 6600, Loss: 0.05265972387718078, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 6700, Loss: 0.04339252456359994, Accuracy: 0.65\n",
      "Epoch: 0, Iteration: 6800, Loss: 0.20776395865867156, Accuracy: 0.72\n",
      "Epoch: 0, Iteration: 6900, Loss: 0.018703639613170713, Accuracy: 0.63\n",
      "Epoch: 0, Iteration: 7000, Loss: 0.03147596204114097, Accuracy: 0.73\n",
      "Epoch: 0, Iteration: 7100, Loss: 0.02575441951196572, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 7200, Loss: 0.01362015242739533, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 7300, Loss: 0.033210303648370146, Accuracy: 0.72\n",
      "Epoch: 0, Iteration: 7400, Loss: 0.027924392410199672, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 7500, Loss: 0.5639757512754192, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 7600, Loss: 0.1549240959512333, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 7700, Loss: 0.028593136070905224, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 7800, Loss: 0.08183683667094699, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 7900, Loss: 0.008394549728061721, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 8000, Loss: 0.26821092804175023, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 8100, Loss: 0.0002158690363585518, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 8200, Loss: 0.011992319885477071, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 8300, Loss: 0.022170984957747712, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 8400, Loss: 0.08205003834672286, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 8500, Loss: 0.1162641986062752, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 8600, Loss: 0.08440277370947472, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 8700, Loss: 0.2780856664127792, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 8800, Loss: 0.004232260873030207, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 8900, Loss: 0.031275813489588564, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 9000, Loss: 0.09387785663301312, Accuracy: 0.66\n",
      "Epoch: 0, Iteration: 9100, Loss: 0.0063798122554921234, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 9200, Loss: 0.16562092289072933, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 9300, Loss: 0.0004214918267828866, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 9400, Loss: 0.010279637439512813, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 9500, Loss: 0.14379373877915946, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 9600, Loss: 0.054192736786335315, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 9700, Loss: 0.21208856024083816, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 9800, Loss: 0.016888158557844497, Accuracy: 0.73\n",
      "Epoch: 0, Iteration: 9900, Loss: 0.09897843684592357, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 10000, Loss: 0.008120423368834544, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 10100, Loss: 0.013060419063406775, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 10200, Loss: 0.09027454664529842, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 10300, Loss: 0.04783156271947832, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 10400, Loss: 0.19113661753746175, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 10500, Loss: 0.03913178887180474, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 10600, Loss: 0.22466334527981885, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 10700, Loss: 0.7481527760775004, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 10800, Loss: 0.006440869733941162, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 10900, Loss: 0.01507373592838423, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 11000, Loss: 0.027156581447572964, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 11100, Loss: 0.00045318880032862286, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 11200, Loss: 0.006794328216217217, Accuracy: 0.73\n",
      "Epoch: 0, Iteration: 11300, Loss: 0.012447820986557234, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 11400, Loss: 0.04650713884573444, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 11500, Loss: 0.021470614748931544, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 11600, Loss: 0.004565791664037309, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 11700, Loss: 0.0031649998510550866, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 11800, Loss: 0.030519065519469535, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 11900, Loss: 0.013019587523114839, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 12000, Loss: 0.009618124225356139, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 12100, Loss: 0.00034188543358516484, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 12200, Loss: 1.871753264509414, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 12300, Loss: 0.5722772016621225, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 12400, Loss: 0.007413818629040864, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 12500, Loss: 0.43455674540613565, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 12600, Loss: 0.6212445764058565, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 12700, Loss: 0.21407668526098353, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 12800, Loss: 0.006875307485997829, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 12900, Loss: 0.037915863675983805, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 13000, Loss: 0.008246211724610309, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 13100, Loss: 0.003145425108320516, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 13200, Loss: 1.4925570952373453, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 13300, Loss: 0.20354620957571923, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 13400, Loss: 0.03946266375296081, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 13500, Loss: 0.49643358355952527, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 13600, Loss: 0.27254972170960423, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 13700, Loss: 0.023019979509446873, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 13800, Loss: 0.12720208935779168, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 13900, Loss: 0.005177963944727823, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 14000, Loss: 0.01961485748874597, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 14100, Loss: 0.025388521579145524, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 14200, Loss: 0.005399641056577625, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 14300, Loss: 0.05432051314837255, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 14400, Loss: 0.32318282536861775, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 14500, Loss: 0.022620567195470353, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 14600, Loss: 0.4686686869015079, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 14700, Loss: 0.06291853346239809, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 14800, Loss: 0.033865303578867585, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 14900, Loss: 0.025354738809312004, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 15000, Loss: 0.0018052479203970405, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 15100, Loss: 0.02379510794470261, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 15200, Loss: 0.024880390556363426, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 15300, Loss: 0.09449597410745204, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 15400, Loss: 0.26018540899630205, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 15500, Loss: 0.30192729242205396, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 15600, Loss: 0.013571658609225457, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 15700, Loss: 0.007106708726420966, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 15800, Loss: 0.44835823267823854, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 15900, Loss: 0.12505826990348773, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 16000, Loss: 0.02290600175244382, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16100, Loss: 0.18064747758033903, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 16200, Loss: 0.05207944739161123, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 16300, Loss: 0.08244450266408632, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 16400, Loss: 0.003073588511427137, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16500, Loss: 0.1103458343267997, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16600, Loss: 0.0018717701686342184, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16700, Loss: 0.020309664207114837, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 16800, Loss: 0.048962392674177395, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 16900, Loss: 0.34869283518371674, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 17000, Loss: 0.07132632045262362, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 17100, Loss: 0.026241207199946992, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 17200, Loss: 0.11144788578576388, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 17300, Loss: 0.0800297821548414, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 17400, Loss: 0.003822815956596092, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 17500, Loss: 0.30891342201664096, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 17600, Loss: 0.00020972869860564226, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 17700, Loss: 0.04242977267018721, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 17800, Loss: 0.0034061484735977967, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 17900, Loss: 0.059849275571717084, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 18000, Loss: 0.15771756602797704, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 18100, Loss: 0.002973452036033439, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 18200, Loss: 0.01789582080003587, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 18300, Loss: 0.006252356376377216, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 18400, Loss: 0.004463822282200497, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 18500, Loss: 0.00040655892714818777, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 18600, Loss: 0.13061807112725282, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 18700, Loss: 0.007221046063113494, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 18800, Loss: 0.008713091302044182, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 18900, Loss: 0.0669692732886448, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 19000, Loss: 0.34796338491917156, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 19100, Loss: 0.0006307807206212589, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 19200, Loss: 0.006168351882795873, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 19300, Loss: 0.08516675370978917, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 19400, Loss: 0.0060983453624798775, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 19500, Loss: 0.00017602650723704541, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 19600, Loss: 0.016444472878691923, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 19700, Loss: 0.03630590981184442, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 19800, Loss: 0.11088374000996204, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 19900, Loss: 0.008894514276621685, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 20000, Loss: 0.02254982031478193, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 20100, Loss: 0.010925106155848362, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 20200, Loss: 0.13963707130700145, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 20300, Loss: 0.006479112112187856, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 20400, Loss: 0.0001580461564391022, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 20500, Loss: 0.012017371554769049, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 20600, Loss: 0.04292485720839007, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 20700, Loss: 0.00018819475632065653, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 20800, Loss: 0.0001339988935164298, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 20900, Loss: 0.002923114841774856, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 21000, Loss: 0.004926783933278322, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 21100, Loss: 0.010440440989937566, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 21200, Loss: 0.01859424509428121, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 21300, Loss: 0.0004151249922383373, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 21400, Loss: 0.000858785293247059, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 21500, Loss: 0.0002718489741356237, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 21600, Loss: 0.024749856278620013, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 21700, Loss: 0.21061309325639904, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 21800, Loss: 0.01765966189982983, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 21900, Loss: 0.4724848356903103, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 22000, Loss: 0.0020360435917123605, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 22100, Loss: 0.14436426796794466, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 22200, Loss: 0.05372524842369382, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 22300, Loss: 0.06905824304635756, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 22400, Loss: 0.030363442701283584, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 22500, Loss: 0.06113620759094168, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 22600, Loss: 0.005687315777684072, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 22700, Loss: 0.014869257236705031, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 22800, Loss: 0.16684679286203613, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 22900, Loss: 0.16516052128233744, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 23000, Loss: 0.0009547010795013227, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 23100, Loss: 0.025922505506127864, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 23200, Loss: 0.2011940541432649, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 23300, Loss: 0.0016725930860921653, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 23400, Loss: 0.00099958791850794, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 23500, Loss: 0.0006540873920767703, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 23600, Loss: 0.0699878418058295, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 23700, Loss: 0.005178932957989297, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 23800, Loss: 0.10955474721117801, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 23900, Loss: 0.00036557167685893915, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 24000, Loss: 0.07932916839415585, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 24100, Loss: 0.001444663784985522, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 24200, Loss: 0.0007036622557004808, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 24300, Loss: 0.06102945479374021, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 24400, Loss: 0.00967960120316795, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 24500, Loss: 0.00037697899517921473, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 24600, Loss: 0.011679038234632897, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 24700, Loss: 0.00601282329224486, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 24800, Loss: 0.0013339654977338965, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 24900, Loss: 0.00048301138602439, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 25000, Loss: 0.004664331784149279, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 25100, Loss: 0.00011068060316668786, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 25200, Loss: 0.09635046904805891, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 25300, Loss: 0.03913965495361183, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 25400, Loss: 0.01615379515888552, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 25500, Loss: 0.036350616578606165, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 25600, Loss: 0.00015140284996500685, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 25700, Loss: 0.002373228817498788, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 25800, Loss: 0.033254943981939414, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 25900, Loss: 0.00012578446543857, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 26000, Loss: 0.03952871603057413, Accuracy: 0.94\n",
      "Epoch: 0, Iteration: 26100, Loss: 0.015021213737783595, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 26200, Loss: 0.000759689710177732, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 26300, Loss: 0.6279295389553374, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 26400, Loss: 0.0019416068828550692, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 26500, Loss: 8.19140335879717e-05, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 26600, Loss: 0.0002639793713065424, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 26700, Loss: 0.020514219947689177, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 26800, Loss: 0.002985388610046559, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 26900, Loss: 0.001937987152966523, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 27000, Loss: 0.004439794527998437, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 27100, Loss: 0.007071732485285645, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 27200, Loss: 0.001570234527739117, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 27300, Loss: 0.004842248986204521, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 27400, Loss: 0.13949278722468367, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 27500, Loss: 0.0560892185845059, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 27600, Loss: 0.001801092139144021, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 27700, Loss: 0.0032655128107318618, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 27800, Loss: 0.13088051399860876, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 27900, Loss: 0.026930762735155434, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 28000, Loss: 0.0033506911385877782, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 28100, Loss: 0.006383090153543341, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 28200, Loss: 0.0360411583152885, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 28300, Loss: 0.004346419084294552, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 28400, Loss: 6.672326965676357e-05, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 28500, Loss: 0.0028389510742481134, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 28600, Loss: 0.006875813745685825, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 28700, Loss: 0.058234839595771845, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 28800, Loss: 0.9681324801145278, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 28900, Loss: 0.04180625660118084, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 29000, Loss: 0.005888442429651062, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 29100, Loss: 0.0035973552579801546, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 29200, Loss: 0.10813018244414492, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 29300, Loss: 0.004311594854319021, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 29400, Loss: 0.07723032548125017, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 29500, Loss: 0.1756912156033128, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 29600, Loss: 0.01876457335689876, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 29700, Loss: 0.009103450891194388, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 29800, Loss: 0.019038992668331606, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 29900, Loss: 0.002154905262202934, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 30000, Loss: 0.30651701858047675, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 30100, Loss: 0.00033252328675779637, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 30200, Loss: 0.0037932427095640894, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 30300, Loss: 0.048259588503032046, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 30400, Loss: 0.0007788277040502022, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 30500, Loss: 0.004436663671120099, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 30600, Loss: 0.005528047052216678, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 30700, Loss: 0.021319035680180377, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 30800, Loss: 0.0018852889060485406, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 30900, Loss: 0.04733001864271596, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 31000, Loss: 0.161611543127447, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 31100, Loss: 0.000498723489492423, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 31200, Loss: 0.0017346772393830282, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 31300, Loss: 0.6029372464158057, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 31400, Loss: 0.0009963238006082487, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 31500, Loss: 0.0002926743037362467, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 31600, Loss: 0.019408194595146165, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 31700, Loss: 0.026641254204533015, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 31800, Loss: 0.007932894059491033, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 31900, Loss: 0.014624223408655082, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 32000, Loss: 0.3668712485867548, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 32100, Loss: 0.5903099960432241, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 32200, Loss: 0.007357382508134044, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 32300, Loss: 0.24240338386982732, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 32400, Loss: 0.0012884759808616776, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 32500, Loss: 0.00016906963151765945, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 32600, Loss: 9.713602231031951e-05, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 32700, Loss: 0.012153587670717681, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 32800, Loss: 0.000570070918467638, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 32900, Loss: 0.22237878559723095, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 33000, Loss: 0.0004641456599109809, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 33100, Loss: 0.004951456322195427, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 33200, Loss: 8.512983287021513e-05, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 33300, Loss: 0.0008596242281246854, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 33400, Loss: 0.00537964446211986, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 33500, Loss: 0.14962163584538163, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 33600, Loss: 0.015519440012921156, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 33700, Loss: 0.022129660111736026, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 33800, Loss: 0.40689110267425177, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 33900, Loss: 0.007903144831373294, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 34000, Loss: 0.00823704214888514, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 34100, Loss: 0.003463912345652846, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 34200, Loss: 0.00022245330703324494, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 34300, Loss: 0.005848042596093727, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 34400, Loss: 0.0986105315022108, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 34500, Loss: 0.04766968988819344, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 34600, Loss: 0.06646118266175066, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 34700, Loss: 0.0017304110944951167, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 34800, Loss: 0.13329718553366254, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 34900, Loss: 0.014671132153771974, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 35000, Loss: 6.297686014312872e-05, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 35100, Loss: 0.010791914679018135, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 35200, Loss: 0.0016331736534783952, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35300, Loss: 0.012759086452671908, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 35400, Loss: 0.04456374071588643, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35500, Loss: 0.0009371946776959054, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35600, Loss: 0.0003096988495626332, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35700, Loss: 0.00546060857992511, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 35800, Loss: 0.00583217492127691, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 35900, Loss: 0.6203108026495475, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 36000, Loss: 0.028886904460079264, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 36100, Loss: 0.004697376937527182, Accuracy: 0.94\n",
      "Epoch: 0, Iteration: 36200, Loss: 0.17018699365829298, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 36300, Loss: 0.07173446270590693, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 36400, Loss: 0.041160578058632866, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 36500, Loss: 0.00019499082187151787, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 36600, Loss: 0.5317793985658558, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 36700, Loss: 0.004933963577637665, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 36800, Loss: 0.047900654738107035, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 36900, Loss: 0.2593535757200941, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 37000, Loss: 0.00026643943275869714, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 37100, Loss: 0.04101764356126339, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 37200, Loss: 0.0018505606341249364, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 37300, Loss: 0.0004961132157607531, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 37400, Loss: 0.07237410788962152, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 37500, Loss: 0.0009144819344885424, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 37600, Loss: 0.5484392345577994, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 37700, Loss: 0.0020269070839818476, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 37800, Loss: 0.007912694156841335, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 37900, Loss: 4.244796141411557e-06, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 38000, Loss: 0.015160828995373188, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 38100, Loss: 0.039262147546530304, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 38200, Loss: 0.0048532965243104804, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 38300, Loss: 0.021840812730844168, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 38400, Loss: 0.01486502759282537, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 38500, Loss: 0.0007585615091490598, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 38600, Loss: 0.0033166055394646204, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 38700, Loss: 0.18645274482185353, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 38800, Loss: 0.010145828509793332, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 38900, Loss: 0.10949238815570297, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 39000, Loss: 0.0039966304759349956, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 39100, Loss: 0.04088861777627041, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 39200, Loss: 0.031489454818251585, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 39300, Loss: 0.0019434306884944439, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 39400, Loss: 0.06375364736331243, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 39500, Loss: 0.27647275252126874, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 39600, Loss: 0.0029489226019530513, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 39700, Loss: 0.0011437585455171856, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 39800, Loss: 0.0008929687468553421, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 39900, Loss: 0.3444166708865425, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 40000, Loss: 0.509652946546748, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 40100, Loss: 0.055885926034810976, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 40200, Loss: 0.0007489274063751253, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 40300, Loss: 0.024805045361794304, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 40400, Loss: 0.016282210282778257, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 40500, Loss: 0.03574264399355574, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 40600, Loss: 0.0013169894260492362, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 40700, Loss: 0.00755932375814401, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 40800, Loss: 0.3177714474672682, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 40900, Loss: 0.028949504003470482, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 41000, Loss: 0.000389110786952103, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 41100, Loss: 0.014972795698833896, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 41200, Loss: 2.5392378021529752e-05, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 41300, Loss: 0.5275524317216377, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 41400, Loss: 0.023717621052152197, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 41500, Loss: 0.0034771585377887408, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 41600, Loss: 0.029305559002466668, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 41700, Loss: 0.14022514686467108, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 41800, Loss: 0.004841835238440647, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 41900, Loss: 0.029320141168847957, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 42000, Loss: 0.00016548394154475662, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 42100, Loss: 0.003979780505688483, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 42200, Loss: 0.040687999221595175, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 42300, Loss: 0.0026378344178080654, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 42400, Loss: 0.0006266381897515941, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 42500, Loss: 0.007376376803056151, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 42600, Loss: 0.023265124501565787, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 42700, Loss: 0.055183486292299036, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 42800, Loss: 0.13910947501969576, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 42900, Loss: 0.2557011488287074, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 43000, Loss: 0.00045276121624860267, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 43100, Loss: 0.002721416343515579, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 43200, Loss: 0.009525908400169704, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 43300, Loss: 0.43661848266618897, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 43400, Loss: 0.07917627341158354, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 43500, Loss: 0.0026456149191752608, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 43600, Loss: 0.04110527073139861, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 43700, Loss: 0.008969728365522046, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 43800, Loss: 0.08612181582033651, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 43900, Loss: 0.42319405446575287, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 44000, Loss: 2.1748797204870195e-05, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 44100, Loss: 0.02057867504240792, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 44200, Loss: 0.000663461831479001, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 44300, Loss: 0.008589039373103659, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 44400, Loss: 0.015528009088043835, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 44500, Loss: 0.04943005089037418, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 44600, Loss: 0.02792873330215554, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 44700, Loss: 0.12722762893310285, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 44800, Loss: 0.0001389614838168361, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 44900, Loss: 0.08814567102866334, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 45000, Loss: 0.10756960836563778, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 45100, Loss: 0.4798681320437222, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 45200, Loss: 0.000286375272873295, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 45300, Loss: 0.19545823192128572, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 45400, Loss: 0.0012688990257121887, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 45500, Loss: 0.0319781434720514, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 45600, Loss: 0.00854449441090977, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 45700, Loss: 0.056209676434384306, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 45800, Loss: 0.0003590982264220987, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 45900, Loss: 0.010258421303887618, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 46000, Loss: 0.009819790668422628, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 46100, Loss: 0.002020244086095535, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 46200, Loss: 0.004618354861910238, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 46300, Loss: 0.19731010150278788, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 46400, Loss: 0.0012550866010851373, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 46500, Loss: 0.0030601535527749434, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 46600, Loss: 0.1355931198409232, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 46700, Loss: 0.0014637085940231473, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 46800, Loss: 0.028548462077657773, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 46900, Loss: 0.00011038997056432638, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 47000, Loss: 0.09764683240637295, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 47100, Loss: 0.0011250546229216588, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 47200, Loss: 0.0011497203976501588, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 47300, Loss: 0.04889997279464144, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 47400, Loss: 0.004967814828649937, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 47500, Loss: 0.0015544976277850305, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 47600, Loss: 0.004608996464633909, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 47700, Loss: 0.30967320437447887, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 47800, Loss: 0.0012748225034455458, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 47900, Loss: 0.12058933170008383, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 48000, Loss: 5.9125456201806556e-05, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 48100, Loss: 0.04096805928747256, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 48200, Loss: 1.1208253302649036, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 48300, Loss: 0.004678684853098509, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 48400, Loss: 0.0011150197515747394, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 48500, Loss: 0.0002649223372293236, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 48600, Loss: 0.0006054342910541766, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 48700, Loss: 0.0007942031143003002, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 48800, Loss: 0.00983118969051489, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 48900, Loss: 0.0037004592344059826, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 49000, Loss: 0.0020654221798754075, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 49100, Loss: 0.29203749167312926, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 49200, Loss: 0.0002534313080154375, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 49300, Loss: 0.007497517103291761, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 49400, Loss: 0.0001751552038284522, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 49500, Loss: 0.004789557847884323, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 49600, Loss: 0.0008511767596087168, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 49700, Loss: 0.0007694060736916236, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 49800, Loss: 0.039869284493597196, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 49900, Loss: 0.012508632727321839, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 50000, Loss: 0.19174465495744372, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 50100, Loss: 0.0007868717849621022, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 50200, Loss: 0.0002903953434636341, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 50300, Loss: 0.0002119229961435218, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 50400, Loss: 0.0034199470235285975, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 50500, Loss: 0.042648047737523634, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 50600, Loss: 0.012379982232759667, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 50700, Loss: 0.005679887928654754, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 50800, Loss: 0.00932330192854543, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 50900, Loss: 0.39098804080925187, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 51000, Loss: 0.5225199688135551, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 51100, Loss: 9.242945276957103e-05, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 51200, Loss: 0.03489081747510915, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 51300, Loss: 7.271447216232056e-06, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 51400, Loss: 0.0003703611770975994, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 51500, Loss: 0.07666004453261385, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 51600, Loss: 0.0043187639736129955, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 51700, Loss: 0.002033398273535489, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 51800, Loss: 0.011716232870745158, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 51900, Loss: 0.1831272826071535, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 52000, Loss: 0.0001838068049452324, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 52100, Loss: 0.0008624570840823383, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 52200, Loss: 0.004514160686136021, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 52300, Loss: 0.0015867152976784295, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 52400, Loss: 0.09934176866712024, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 52500, Loss: 0.004044572194187233, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 52600, Loss: 0.006642030828570333, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 52700, Loss: 0.00013848860921467202, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 52800, Loss: 0.00037968451561383747, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 52900, Loss: 0.00013954844821128186, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 53000, Loss: 0.019006146778291672, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 53100, Loss: 0.02506171480801684, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 53200, Loss: 0.0009189359297451837, Accuracy: 0.96\n",
      "Epoch: 0, Iteration: 53300, Loss: 0.5837980281935506, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 53400, Loss: 0.006946161802294836, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 53500, Loss: 0.004890329871717923, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 53600, Loss: 0.01076317945139405, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 53700, Loss: 0.0027141213469500866, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 53800, Loss: 0.0018374250500386845, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 53900, Loss: 0.0016977439881193106, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 54000, Loss: 0.020719882394024926, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 54100, Loss: 0.024493028454575023, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 54200, Loss: 0.05347716780522943, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 54300, Loss: 0.02159266734674963, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 54400, Loss: 0.007168502177308561, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 54500, Loss: 0.09009374216459894, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 54600, Loss: 0.006680610054038037, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 54700, Loss: 0.09524193902161482, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 54800, Loss: 0.002727853138904771, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 54900, Loss: 0.020741569238615854, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 55000, Loss: 0.0013480133367244193, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 55100, Loss: 0.0003669786771856607, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 55200, Loss: 0.7513881277656879, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 55300, Loss: 0.0059976058358299725, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 55400, Loss: 0.004009452321844729, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 55500, Loss: 0.0003751792970326519, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 55600, Loss: 0.0006791562151750895, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 55700, Loss: 0.0018869399935107888, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 55800, Loss: 0.003181564009697972, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 55900, Loss: 0.012110945629554152, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 56000, Loss: 0.002483904230626942, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 56100, Loss: 0.010999163917089326, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 56200, Loss: 0.00017718079963372104, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 56300, Loss: 0.0038861311767108483, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 56400, Loss: 0.1117766074433465, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 56500, Loss: 8.010389085291318e-05, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 56600, Loss: 0.18028402958298623, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 56700, Loss: 0.0593875159813422, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 56800, Loss: 0.008084230458742729, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 56900, Loss: 0.00019789196260479774, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 57000, Loss: 0.0007379942075373689, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 57100, Loss: 0.001478200397546887, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 57200, Loss: 0.0009145376965133222, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 57300, Loss: 0.5216333275004248, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 57400, Loss: 6.03652005705888e-05, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 57500, Loss: 0.0022087967267277885, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 57600, Loss: 0.01919595327076632, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 57700, Loss: 2.740258552188866e-05, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 57800, Loss: 0.00044476456235860527, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 57900, Loss: 0.004296654288871191, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 58000, Loss: 0.0023348386504760546, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 58100, Loss: 0.0059665640696196015, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 58200, Loss: 2.738589111165005e-05, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 58300, Loss: 0.0015598173194389324, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 58400, Loss: 0.000246697907643035, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 58500, Loss: 0.0006597188874288867, Accuracy: 0.94\n",
      "Epoch: 0, Iteration: 58600, Loss: 0.0073954283955124794, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 58700, Loss: 0.005654333288677203, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 58800, Loss: 1.3571885046424685, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 58900, Loss: 0.00039006243064833486, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 59000, Loss: 0.20444393023640356, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 59100, Loss: 0.004107796948233497, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 59200, Loss: 0.03206967087450345, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 59300, Loss: 0.004507142156221501, Accuracy: 0.94\n",
      "Epoch: 0, Iteration: 59400, Loss: 0.0003843583198949261, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 59500, Loss: 9.131927816082576e-05, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 59600, Loss: 0.004586447004707164, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 59700, Loss: 0.0240412506181564, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 59800, Loss: 0.3385529274781346, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 59900, Loss: 0.007716156494776942, Accuracy: 0.92\n",
      "Epoch: 0, Test Accuracy: 0.8536\n",
      "Epoch: 1, Iteration: 0, Loss: 0.021728805817447065, Accuracy: 0.01\n",
      "Epoch: 1, Iteration: 100, Loss: 0.10199666244522318, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 200, Loss: 0.0009291819320963901, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 300, Loss: 0.0005109696101007851, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 400, Loss: 2.4990327195255183e-05, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 500, Loss: 0.00039612850650439034, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 600, Loss: 0.00048701906357958734, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 700, Loss: 0.0048473561807600525, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 800, Loss: 0.00027277816537077686, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 900, Loss: 0.03475375527936105, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 1000, Loss: 0.0028875378777188706, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 1100, Loss: 0.0010186477745901287, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 1200, Loss: 0.00023365111459914805, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 1300, Loss: 0.0015026082764168956, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 1400, Loss: 2.9313087647686244e-05, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 1500, Loss: 2.9382817552414908e-05, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 1600, Loss: 0.010857270745189954, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 1700, Loss: 0.006472161899551595, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 1800, Loss: 0.0016295366811644237, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 1900, Loss: 0.05478443562913014, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 2000, Loss: 1.3220200116614942e-05, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 2100, Loss: 0.00559625747009075, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 2200, Loss: 0.003918286900480872, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 2300, Loss: 0.03146537596561205, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 2400, Loss: 0.03962332460028138, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 2500, Loss: 0.03198735422283638, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 2600, Loss: 0.0013486424661679948, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 2700, Loss: 0.0639312439346669, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 2800, Loss: 0.06823015056756716, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 2900, Loss: 0.08437957620833487, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 3000, Loss: 0.17784765011035503, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 3100, Loss: 0.00014262172110099072, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 3200, Loss: 0.39738279724571596, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 3300, Loss: 0.7035572049138279, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 3400, Loss: 0.0026853493125394584, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 3500, Loss: 5.8416201826197876e-05, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 3600, Loss: 0.057276298929449555, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 3700, Loss: 0.009285988263051572, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 3800, Loss: 0.0016984446335398845, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 3900, Loss: 0.001490453727771229, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 4000, Loss: 0.04304145072546626, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 4100, Loss: 0.0004449048439492475, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 4200, Loss: 0.021840235934079574, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 4300, Loss: 0.03282591127196193, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 4400, Loss: 0.0007260320270329205, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 4500, Loss: 0.00023267498044343135, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 4600, Loss: 0.0007587036062477668, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 4700, Loss: 0.00018650782457761616, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 4800, Loss: 0.0028762516935768507, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 4900, Loss: 0.021055842044740578, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 5000, Loss: 0.0006856457575443477, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 5100, Loss: 0.007145324946498515, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 5200, Loss: 0.007050974409618377, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 5300, Loss: 0.01212873055739043, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 5400, Loss: 0.8259505109906641, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 5500, Loss: 4.898804576622985e-05, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 5600, Loss: 0.014954642380542338, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 5700, Loss: 0.004750936724511174, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 5800, Loss: 0.006246064020246669, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 5900, Loss: 0.034972723623859635, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 6000, Loss: 0.00173271981615422, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 6100, Loss: 0.02854598587443044, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 6200, Loss: 0.35662016292972193, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 6300, Loss: 0.02907977992075919, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 6400, Loss: 0.007209574410816953, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 6500, Loss: 0.004791667116853887, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 6600, Loss: 0.001952294153428544, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 6700, Loss: 0.005245638862951793, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 6800, Loss: 0.0016400367435182074, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 6900, Loss: 0.001540688004152745, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 7000, Loss: 1.1130775666429817e-05, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 7100, Loss: 0.008306596095348626, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 7200, Loss: 0.0002247616534049736, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 7300, Loss: 0.0015954592497562225, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 7400, Loss: 0.0015556700392283854, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 7500, Loss: 0.45780227425890374, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 7600, Loss: 0.002795325909728007, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 7700, Loss: 0.0019409628697209097, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 7800, Loss: 0.020758308197186725, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 7900, Loss: 0.0012799178486126608, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 8000, Loss: 0.01942527791357043, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 8100, Loss: 6.469934591298645e-06, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 8200, Loss: 0.0002669150743287241, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 8300, Loss: 0.00011776002647457133, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 8400, Loss: 0.004554251830257546, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 8500, Loss: 0.05157098166155938, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 8600, Loss: 0.0024551066042247725, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 8700, Loss: 0.016496978813353748, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 8800, Loss: 0.0002132486361744485, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 8900, Loss: 0.002521589912951151, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 9000, Loss: 0.05061364721578263, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 9100, Loss: 0.00039057834165578324, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 9200, Loss: 0.0013584131455021164, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 9300, Loss: 1.1331037826478038e-06, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 9400, Loss: 0.005248683030010589, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 9500, Loss: 0.002898528301527999, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 9600, Loss: 0.11862053385927585, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 9700, Loss: 0.2090738036842356, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 9800, Loss: 0.01632832400298772, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 9900, Loss: 0.00026836850220424765, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 10000, Loss: 0.0005057317730426554, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 10100, Loss: 0.002099631308354365, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 10200, Loss: 0.15759015696610373, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 10300, Loss: 0.03055514312850884, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 10400, Loss: 0.012496261872437369, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 10500, Loss: 0.0008316515355081012, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 10600, Loss: 0.022411634294104716, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 10700, Loss: 0.7943313776057808, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 10800, Loss: 0.00011402310557619582, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 10900, Loss: 0.0016896022258388091, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 11000, Loss: 3.2522675311064106e-05, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 11100, Loss: 4.2589309875709306e-06, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 11200, Loss: 0.005124748845611752, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 11300, Loss: 0.00037135294347071717, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 11400, Loss: 0.014312558267966135, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 11500, Loss: 0.0006461172240499932, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 11600, Loss: 0.003857550156769018, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 11700, Loss: 0.0065239022637196276, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 11800, Loss: 0.007203839678639232, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 11900, Loss: 0.00062473187145541, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 12000, Loss: 0.007163064147822168, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 12100, Loss: 3.187954259350472e-05, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 12200, Loss: 2.4248761093241984, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 12300, Loss: 0.26419784569477933, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 12400, Loss: 0.0012491319864284395, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 12500, Loss: 0.3276451102675478, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 12600, Loss: 0.6024774086704274, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 12700, Loss: 0.24250047671521355, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 12800, Loss: 0.002703932204195192, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 12900, Loss: 0.008800137964459578, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 13000, Loss: 0.0071693978686109645, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 13100, Loss: 0.0007097953546705419, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 13200, Loss: 1.510978869538042, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 13300, Loss: 0.06262185804068772, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 13400, Loss: 0.005402231909636767, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 13500, Loss: 0.14058755189344369, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 13600, Loss: 0.23011238184916477, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 13700, Loss: 0.000698181970075171, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 13800, Loss: 0.034716439228951984, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 13900, Loss: 0.0008098769880320667, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 14000, Loss: 0.0022180127545439322, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 14100, Loss: 0.008408957615653632, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 14200, Loss: 0.003022945058636326, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 14300, Loss: 0.02521854495304384, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 14400, Loss: 0.08697890793479532, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 14500, Loss: 0.024782796092116057, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 14600, Loss: 0.4903416043177253, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 14700, Loss: 0.0316011284473799, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 14800, Loss: 0.01017369336857143, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 14900, Loss: 0.004490066020206154, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 15000, Loss: 0.00012484574119818744, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 15100, Loss: 0.0017833078517401982, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 15200, Loss: 0.005893760752775351, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 15300, Loss: 0.24931178116579295, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 15400, Loss: 0.06603915743713443, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 15500, Loss: 0.15283737901867006, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 15600, Loss: 0.0021855539854805693, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 15700, Loss: 0.00046300583060501446, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 15800, Loss: 0.3768008567074173, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 15900, Loss: 0.03508013260716487, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 16000, Loss: 0.0005131759431286395, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 16100, Loss: 0.07814539387280828, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 16200, Loss: 0.046692086821252025, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 16300, Loss: 0.15164827414076104, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 16400, Loss: 0.0007307625376415964, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 16500, Loss: 0.003935513295140984, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 16600, Loss: 0.00014872259809056238, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 16700, Loss: 0.06831270291357791, Accuracy: 0.97\n",
      "Epoch: 1, Iteration: 16800, Loss: 0.0026480708352828357, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 16900, Loss: 0.034546104591887304, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 17000, Loss: 0.0019659254523338244, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 17100, Loss: 0.0025750550987234597, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 17200, Loss: 0.04736274079588803, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 17300, Loss: 0.009457718780106341, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 17400, Loss: 0.00046717076859656225, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 17500, Loss: 0.08182026798695546, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 17600, Loss: 1.1471735587141109e-05, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 17700, Loss: 0.0350830876278576, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 17800, Loss: 0.002242626081011545, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 17900, Loss: 0.10658570306838992, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 18000, Loss: 0.03204524029251653, Accuracy: 0.82\n",
      "Epoch: 1, Iteration: 18100, Loss: 0.0009275913499899686, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 18200, Loss: 0.0004331641376606159, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 18300, Loss: 0.0029904581132575896, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 18400, Loss: 0.0014919202659758877, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 18500, Loss: 0.0003858642680587655, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 18600, Loss: 0.014469537061371432, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 18700, Loss: 0.001791736828190465, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 18800, Loss: 0.0021558128029253447, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 18900, Loss: 0.02644248592626755, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 19000, Loss: 0.22843527645708414, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 19100, Loss: 8.731362122684457e-05, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 19200, Loss: 0.0022673730319187958, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 19300, Loss: 0.08176115559874383, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 19400, Loss: 0.004020935022513304, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 19500, Loss: 2.9066918477515922e-05, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 19600, Loss: 0.018003287666942258, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 19700, Loss: 0.00969156844759491, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 19800, Loss: 0.008720992419792756, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 19900, Loss: 0.001383266827607331, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 20000, Loss: 0.0018149470529828244, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 20100, Loss: 0.004653307828395483, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 20200, Loss: 0.018531080589996966, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 20300, Loss: 0.003208917771206808, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 20400, Loss: 8.077166997959807e-06, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 20500, Loss: 0.005382310168222801, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 20600, Loss: 0.011732195990441213, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 20700, Loss: 3.2907342606965667e-06, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 20800, Loss: 0.00014720926278795704, Accuracy: 0.82\n",
      "Epoch: 1, Iteration: 20900, Loss: 0.00652235661396951, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 21000, Loss: 0.0006835294031583404, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 21100, Loss: 0.0008133737207706348, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 21200, Loss: 0.005512914885244026, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 21300, Loss: 8.221550331775999e-05, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 21400, Loss: 0.0007678676533602263, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 21500, Loss: 7.352495303655895e-05, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 21600, Loss: 0.015110959179870192, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 21700, Loss: 0.03528722934392487, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 21800, Loss: 0.01205040066640105, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 21900, Loss: 0.7060899349005716, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 22000, Loss: 0.0004689662854781564, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 22100, Loss: 0.05809839666176538, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 22200, Loss: 0.008079058048327181, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 22300, Loss: 0.023616435723484205, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 22400, Loss: 0.0166941629940241, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 22500, Loss: 0.053216814492768404, Accuracy: 0.82\n",
      "Epoch: 1, Iteration: 22600, Loss: 0.0018403510906226301, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 22700, Loss: 0.002870528085484384, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 22800, Loss: 0.262219721777878, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 22900, Loss: 0.06690987947267832, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 23000, Loss: 0.00028559690607252007, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 23100, Loss: 0.011953011692316015, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 23200, Loss: 0.03882257082526823, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 23300, Loss: 0.005239025138037173, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 23400, Loss: 0.0006067310118945425, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 23500, Loss: 0.00017912990873252503, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 23600, Loss: 0.026312837425485198, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 23700, Loss: 0.0020475656101151232, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 23800, Loss: 0.07139110547793688, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 23900, Loss: 0.00030576600611854096, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 24000, Loss: 0.031117346645964817, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 24100, Loss: 0.0010580367583691186, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 24200, Loss: 0.00029422090140594277, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 24300, Loss: 0.037107647580197425, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 24400, Loss: 0.002920524651985407, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 24500, Loss: 0.00022876306838911877, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 24600, Loss: 0.01125556454242175, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 24700, Loss: 0.0012434358915808619, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 24800, Loss: 0.00010343067860120368, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 24900, Loss: 0.00021323331082381356, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 25000, Loss: 0.0018164550120942694, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 25100, Loss: 2.7181853709069257e-05, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 25200, Loss: 0.05291439559631019, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 25300, Loss: 0.04307590446044549, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 25400, Loss: 0.011504551547281366, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 25500, Loss: 0.03044700665512639, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 25600, Loss: 1.532387519314183e-05, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 25700, Loss: 0.0009623089042636567, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 25800, Loss: 0.014488607373253656, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 25900, Loss: 5.02517417395465e-05, Accuracy: 0.97\n",
      "Epoch: 1, Iteration: 26000, Loss: 0.015249907447312996, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 26100, Loss: 0.010908486979353848, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 26200, Loss: 0.001648028570219821, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 26300, Loss: 0.4811681231209099, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 26400, Loss: 0.001006656025286894, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 26500, Loss: 6.259213966131101e-05, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 26600, Loss: 0.00010363253622313095, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 26700, Loss: 0.01457392920754395, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 26800, Loss: 0.006518928550300352, Accuracy: 0.97\n",
      "Epoch: 1, Iteration: 26900, Loss: 0.0005032912729663202, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 27000, Loss: 0.0013244630279816347, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 27100, Loss: 0.010142621284162764, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 27200, Loss: 0.0005627737349120237, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 27300, Loss: 0.005415195138990812, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 27400, Loss: 0.020849689090603318, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 27500, Loss: 0.028341143637075843, Accuracy: 0.96\n",
      "Epoch: 1, Iteration: 27600, Loss: 0.0019059079779013947, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 27700, Loss: 0.0008724960033763926, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 27800, Loss: 0.061308039732964656, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 27900, Loss: 0.01772744892113343, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 28000, Loss: 0.0004095998217985647, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 28100, Loss: 0.004866472221611701, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 28200, Loss: 0.013811097755816692, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 28300, Loss: 0.002759683083406981, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 28400, Loss: 5.2736092558981846e-05, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 28500, Loss: 0.0015517335281699798, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 28600, Loss: 0.0044856616666396985, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 28700, Loss: 0.02068809814386651, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 28800, Loss: 1.0603148530439435, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 28900, Loss: 0.019730540618421596, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 29000, Loss: 0.007281759610072257, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 29100, Loss: 0.003024714089551379, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 29200, Loss: 0.04840710986104653, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 29300, Loss: 0.0013313913715463884, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 29400, Loss: 0.034861259130656394, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 29500, Loss: 0.030540776659890488, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 29600, Loss: 0.005713364394937257, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 29700, Loss: 0.0033840611453530385, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 29800, Loss: 0.012291223362464295, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 29900, Loss: 0.0016182137120254007, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 30000, Loss: 0.13836766362420397, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 30100, Loss: 0.00022577980283099993, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 30200, Loss: 0.002953898877935617, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 30300, Loss: 0.02922444029929284, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 30400, Loss: 0.00039874875794697416, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 30500, Loss: 0.0011410539515626338, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 30600, Loss: 0.0016685177595971747, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 30700, Loss: 0.01527141400400241, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 30800, Loss: 0.0014559618072818932, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 30900, Loss: 0.0158412237015192, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 31000, Loss: 0.010628862504944183, Accuracy: 0.83\n",
      "Epoch: 1, Iteration: 31100, Loss: 0.00018799455385870977, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 31200, Loss: 0.00123294353799958, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 31300, Loss: 0.7104826007269682, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 31400, Loss: 0.00039957558078848783, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 31500, Loss: 8.481387594540978e-05, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 31600, Loss: 0.00977257975202973, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 31700, Loss: 0.0026503340989184523, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 31800, Loss: 0.009503817285406443, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 31900, Loss: 0.008843276848572725, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 32000, Loss: 0.2197394092691963, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 32100, Loss: 0.3265515543074774, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 32200, Loss: 0.002140289264168065, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 32300, Loss: 0.09623005425410393, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 32400, Loss: 0.0017973797161239928, Accuracy: 0.85\n",
      "Epoch: 1, Iteration: 32500, Loss: 0.0005647037518936489, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 32600, Loss: 2.5803894597811987e-05, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 32700, Loss: 0.003754874514678306, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 32800, Loss: 0.0002630158508093568, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 32900, Loss: 0.2125354459323087, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 33000, Loss: 0.00011437069841547715, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 33100, Loss: 0.006773567211405343, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 33200, Loss: 8.24335082606693e-06, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 33300, Loss: 0.0005118555314611205, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 33400, Loss: 0.008355461265887218, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 33500, Loss: 0.128168289379694, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 33600, Loss: 0.01678350717561311, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 33700, Loss: 0.00963813466435815, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 33800, Loss: 0.11365968179043791, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 33900, Loss: 0.024523131783318695, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 34000, Loss: 0.007254331842215411, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 34100, Loss: 0.0011728457114579805, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 34200, Loss: 9.220209051002798e-05, Accuracy: 0.96\n",
      "Epoch: 1, Iteration: 34300, Loss: 0.0025624611713012837, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 34400, Loss: 0.025122934099634408, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 34500, Loss: 0.027056837615901497, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 34600, Loss: 0.019978102374688937, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 34700, Loss: 0.0008021898527914637, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 34800, Loss: 0.0388719157511636, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 34900, Loss: 0.009405485051817796, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 35000, Loss: 0.0002118547996004454, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 35100, Loss: 0.004346976026437121, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 35200, Loss: 0.0007912034394034012, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 35300, Loss: 0.004363115398985283, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 35400, Loss: 0.019117434289148562, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 35500, Loss: 0.0007097169365244209, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 35600, Loss: 4.320696124482616e-05, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 35700, Loss: 0.0020862980316386013, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 35800, Loss: 0.002394224346276566, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 35900, Loss: 0.4933693314420685, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 36000, Loss: 0.01460368244527376, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 36100, Loss: 0.0023537704767419335, Accuracy: 0.97\n",
      "Epoch: 1, Iteration: 36200, Loss: 0.0961638394571651, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 36300, Loss: 0.031472564876242164, Accuracy: 0.84\n",
      "Epoch: 1, Iteration: 36400, Loss: 0.0350103054889734, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 36500, Loss: 0.00012877036478997788, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 36600, Loss: 0.5334513726591823, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 36700, Loss: 0.0015701238631278189, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 36800, Loss: 0.025780123897674256, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 36900, Loss: 0.21132393497962032, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 37000, Loss: 0.0006203812339676192, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 37100, Loss: 0.01923378839572986, Accuracy: 0.9\n",
      "Epoch: 1, Iteration: 37200, Loss: 0.001977778628459186, Accuracy: 0.89\n",
      "Epoch: 1, Iteration: 37300, Loss: 0.00012375665661182284, Accuracy: 0.92\n",
      "Epoch: 1, Iteration: 37400, Loss: 0.06925698509765701, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 37500, Loss: 0.0003009049123246364, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 37600, Loss: 0.27079568186217023, Accuracy: 0.86\n",
      "Epoch: 1, Iteration: 37700, Loss: 0.00204183577638569, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 37800, Loss: 0.012227615296147352, Accuracy: 0.95\n",
      "Epoch: 1, Iteration: 37900, Loss: 1.436076638332401e-06, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 38000, Loss: 0.013400816210195555, Accuracy: 0.96\n",
      "Epoch: 1, Iteration: 38100, Loss: 0.04495149221200887, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 38200, Loss: 0.001330301364169458, Accuracy: 0.93\n",
      "Epoch: 1, Iteration: 38300, Loss: 0.002494355505628899, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 38400, Loss: 0.0050552647939106085, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 38500, Loss: 0.0006420513225463923, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 38600, Loss: 0.005470282099601399, Accuracy: 0.91\n",
      "Epoch: 1, Iteration: 38700, Loss: 0.3582641524594073, Accuracy: 0.87\n",
      "Epoch: 1, Iteration: 38800, Loss: 0.006371490706355651, Accuracy: 0.88\n",
      "Epoch: 1, Iteration: 38900, Loss: 0.08918401177616955, Accuracy: 0.94\n",
      "Epoch: 1, Iteration: 39000, Loss: 0.0013699801854956868, Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "epochs = 5\n",
    "learning_rate = 0.1\n",
    "gamma = 0.5\n",
    "interval = 1\n",
    "for epoch in range(epochs):\n",
    "    if epoch > 0 and epoch % interval == 0:\n",
    "        learning_rate = learning_rate * gamma\n",
    "    train_correct = 0\n",
    "    for i in range(len(x_train)):\n",
    "        x = x_train[i]\n",
    "        y = y_train[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        layer0 = model[0].forward(x) # CNN with kernel size 3\n",
    "        layer1 = model[1].forward(layer0) # Sigmoid\n",
    "        layer2 = model[2].forward(layer1) # Average pool\n",
    "        layer3 = model[3].forward(layer2) # Flatten\n",
    "        layer4 = model[4].forward(layer3) # Reshape\n",
    "        layer5 = model[5].forward(layer4) # CNN with kernel size 1\n",
    "        layer6 = model[6].forward(layer5) # Reshape\n",
    "        layer7 = model[7].forward(layer6) # Softmax\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_layer.forward(layer7, y) # Cross entropy\n",
    "        # accuracy\n",
    "        train_correct += isCorrect(layer7, y)\n",
    "        \n",
    "        # Grad\n",
    "        grad = loss_layer.backward(layer7, y, learning_rate) # Cross entropy\n",
    "        grad = model[7].backward(layer6, grad, learning_rate) # Softmax\n",
    "        grad = model[6].backward(layer5, grad, learning_rate) # Reshape\n",
    "        grad = model[5].backward(layer4, grad, learning_rate) # CNN with kernel size 1\n",
    "        grad = model[4].backward(layer3, grad, learning_rate) # Reshape\n",
    "        grad = model[3].backward(layer2, grad, learning_rate) # Flatten\n",
    "        grad = model[2].backward(layer1, grad, learning_rate) # Average pool\n",
    "        grad = model[1].backward(layer0, grad, learning_rate) # Sigmoid\n",
    "        grad = model[0].backward(x, grad, learning_rate) # CNN with kernel size 3\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", Iteration: \" + str(i) + \", Loss: \" + str(loss) + \", Accuracy: \" + str(train_correct / 100))\n",
    "            train_correct = 0\n",
    "            \n",
    "    # Test\n",
    "    test_correct = 0\n",
    "    for i in range(len(x_test)):\n",
    "        x = x_test[i]\n",
    "        y = y_test[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        layer0 = model[0].forward(x) # CNN with kernel size 3\n",
    "        layer1 = model[1].forward(layer0) # Sigmoid\n",
    "        layer2 = model[2].forward(layer1) # Average pool\n",
    "        layer3 = model[3].forward(layer2) # Flatten\n",
    "        layer4 = model[4].forward(layer3) # Reshape\n",
    "        layer5 = model[5].forward(layer4) # CNN with kernel size 1\n",
    "        layer6 = model[6].forward(layer5) # Reshape\n",
    "        layer7 = model[7].forward(layer6) # Softmax\n",
    "        \n",
    "        # accuracy\n",
    "        test_correct += isCorrect(layer7, y)\n",
    "    \n",
    "    print(\"Epoch: \" + str(epoch) + \", Test Accuracy: \" + str(test_correct / len(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
