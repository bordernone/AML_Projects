{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-08 16:42:29.023592: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer:\n",
    "    def __init__(self, kernel_size, number_of_kernels, input_shape):\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        self.input_channels = input_shape[2]\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.number_of_kernels = number_of_kernels\n",
    "        self.kernels = np.random.randn(kernel_size, kernel_size, self.input_channels, number_of_kernels)\n",
    "        \n",
    "        # output shape\n",
    "        self.output_shape = (input_shape[0] - kernel_size + 1, input_shape[1] - kernel_size + 1, number_of_kernels)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "\n",
    "        output = np.zeros(self.output_shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                output[:, :, i] += signal.correlate2d(input[:, :, j], self.kernels[:, :, j, i], mode='valid')\n",
    "        \n",
    "        output += self.biases\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size)\n",
    "        \n",
    "        # Compute gradient with respect to kernels\n",
    "        kernel_grads = np.zeros(self.kernels.shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                kernel_grads[:, :, j, i] = signal.correlate2d(input[:, :, j], gradWRTMyOutput[:, :, i], mode='valid')\n",
    "        \n",
    "        # Compute gradient with respect to biases\n",
    "        bias_grads = gradWRTMyOutput.copy()\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        for j in range(self.input_channels):\n",
    "            for i in range(self.number_of_kernels):\n",
    "                input_grads[:, :, j] += signal.convolve2d(gradWRTMyOutput[:, :, i], self.kernels[:, :, j, i], mode='full')\n",
    "        \n",
    "        # Update kernels and biases\n",
    "        self.kernels -= learning_rate * kernel_grads\n",
    "        self.biases -= learning_rate * bias_grads\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z')\n",
    "\n",
    "Error_Grad_WRT_Input: Error_Grad_WRT_Output * Output_Grad_WRT_Input , where * is element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = 1.0 / (1.0 + np.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at sigmoid layer\"\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at sigmoid layer\"\n",
    "        # element-wise gradient\n",
    "        gradMyOutputWRTMyInput = self.sigmoid_derivative(self.output)\n",
    "        # element-wise multiplication\n",
    "        return gradWRTMyOutput * gradMyOutputWRTMyInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Pooling Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z)\n",
    "\n",
    "Error Gradient\n",
    "\n",
    "$\\frac{dE}{dX_{ijk}} = \\sum_{m,n} \\frac{dE}{dY_{mnk}} \\times \\frac{1}{\\text{pool\\_size}^2} \\times \\mathbb{\\theta}_{i, j}$ where $\\mathbb{\\theta}_{i, j}$ is 1 if $X_{ijk}$ was included when calculating $Y_{mnk}$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePoolLayer:\n",
    "    def __init__(self, input_shape, pool_size, stride):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        # input shape must be divisible by pool size\n",
    "        assert input_shape[0] % pool_size == 0, \"Input shape must be divisible by pool size\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (input_shape[0] // stride, input_shape[1] // stride, input_shape[2])\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "        for i in range(self.output_shape[0]):\n",
    "            for j in range(self.output_shape[1]):\n",
    "                for k in range(self.output_shape[2]):\n",
    "                    self.output[i, j, k] = np.mean(input[i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size, k])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradWRTMyOutput):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at average pool layer\"\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at average pool layer\"\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        avg_pool_n = self.pool_size * self.pool_size # number of elements involved in the average pool operation\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        \n",
    "        for channel in range(self.input_shape[2]):\n",
    "            for i in range(self.output_shape[0]):\n",
    "                for j in range(self.output_shape[1]):\n",
    "                    # Only loop over the elements involved in the average pool operation\n",
    "                    for x in range(i*self.stride, i*self.stride+self.pool_size):\n",
    "                        for y in range(j*self.stride, j*self.stride+self.pool_size):\n",
    "                            input_grads[x, y, channel] += gradWRTMyOutput[i, j, channel] / avg_pool_n\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer():\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (np.prod(input_shape),)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = input.flatten()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at flatten layer\"\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at flatten layer\"\n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Layer\n",
    "Softmax Derivative: $\\frac{dS_i}{dX_j} = S_i(1 - S_i)$ if $i = j$ and $-S_iS_j$ otherwise\n",
    "\n",
    "In matrix form, obtain $C = softmax(x) \\times softmax(x)^T$ and $D = diag(softmax(x))$ where $D$ is a diagonal matrix with the softmax values on the diagonal and 0 elsewhere.\n",
    "\n",
    "$K = D - C$\n",
    "Then, $\\frac{dS}{dX} = K \\times \\frac{dE}{dO}$ where $O$ is the output of the softmax layer and $E$ is the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 1D\n",
    "        assert len(input_shape) == 1, \"Input shape must be 1D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = np.exp(input) / np.sum(np.exp(input))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at softmax layer\"\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at softmax layer\"\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        inp_softmax = self.softmax(input)\n",
    "        softmax_matrix = np.reshape(inp_softmax, (inp_softmax.shape[0], 1)) @ np.reshape(inp_softmax, (1, inp_softmax.shape[0]))\n",
    "        softmax_matrix = -1 * softmax_matrix\n",
    "        np.fill_diagonal(softmax_matrix, inp_softmax * (1 - inp_softmax))\n",
    "        input_grads = softmax_matrix @ gradWRTMyOutput\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Display first image\n",
    "Image.fromarray(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Reshape the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # (height, width, channel)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) # (height, width, channel)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonus-project-vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
