{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer:\n",
    "    def __init__(self, kernel_size, number_of_kernels, input_shape):\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\" \n",
    "        self.input_channels = input_shape[2]\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.number_of_kernels = number_of_kernels\n",
    "        self.kernels = np.random.randn(kernel_size, kernel_size, self.input_channels, number_of_kernels)\n",
    "        \n",
    "        # output shape\n",
    "        self.output_shape = (input_shape[0] - kernel_size + 1, input_shape[1] - kernel_size + 1, number_of_kernels)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. \" + \"Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "\n",
    "        output = np.zeros(self.output_shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                output[:, :, i] += signal.correlate2d(input[:, :, j], self.kernels[:, :, j, i], mode='valid')\n",
    "        \n",
    "        output += self.biases\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size) + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size) + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        \n",
    "        # Compute gradient with respect to kernels\n",
    "        kernel_grads = np.zeros(self.kernels.shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                kernel_grads[:, :, j, i] = signal.correlate2d(input[:, :, j], gradWRTMyOutput[:, :, i], mode='valid')\n",
    "        \n",
    "        # Compute gradient with respect to biases\n",
    "        bias_grads = gradWRTMyOutput.copy()\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        for j in range(self.input_channels):\n",
    "            for i in range(self.number_of_kernels):\n",
    "                input_grads[:, :, j] += signal.convolve2d(gradWRTMyOutput[:, :, i], self.kernels[:, :, j, i], mode='full')\n",
    "        \n",
    "        # Update kernels and biases\n",
    "        self.kernels -= learning_rate * kernel_grads\n",
    "        self.biases -= learning_rate * bias_grads\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z')\n",
    "\n",
    "Error_Grad_WRT_Input: Error_Grad_WRT_Output * Output_Grad_WRT_Input , where * is element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = 1.0 / (1.0 + np.exp(-input))\n",
    "        return self.output\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at sigmoid layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at sigmoid layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        # element-wise gradient\n",
    "        gradMyOutputWRTMyInput = self.sigmoid_derivative(self.output)\n",
    "        # element-wise multiplication\n",
    "        return gradWRTMyOutput * gradMyOutputWRTMyInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Pooling Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z)\n",
    "\n",
    "Error Gradient\n",
    "\n",
    "$\\frac{dE}{dX_{ijk}} = \\sum_{m,n} \\frac{dE}{dY_{mnk}} \\times \\frac{1}{\\text{pool\\_size}^2} \\times \\mathbb{\\theta}_{i, j}$ where $\\mathbb{\\theta}_{i, j}$ is 1 if $X_{ijk}$ was included when calculating $Y_{mnk}$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePoolLayer:\n",
    "    def __init__(self, input_shape, pool_size, stride):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        # input shape must be divisible by pool size\n",
    "        assert input_shape[0] % pool_size == 0, \"Input shape must be divisible by pool size\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (input_shape[0] // stride, input_shape[1] // stride, input_shape[2])\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "        for i in range(self.output_shape[0]):\n",
    "            for j in range(self.output_shape[1]):\n",
    "                for k in range(self.output_shape[2]):\n",
    "                    self.output[i, j, k] = np.mean(input[i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size, k])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at average pool layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at average pool layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        avg_pool_n = self.pool_size * self.pool_size # number of elements involved in the average pool operation\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        \n",
    "        for channel in range(self.input_shape[2]):\n",
    "            for i in range(self.output_shape[0]):\n",
    "                for j in range(self.output_shape[1]):\n",
    "                    # Only loop over the elements involved in the average pool operation\n",
    "                    for x in range(i*self.stride, i*self.stride+self.pool_size):\n",
    "                        for y in range(j*self.stride, j*self.stride+self.pool_size):\n",
    "                            input_grads[x, y, channel] += gradWRTMyOutput[i, j, channel] / avg_pool_n\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer():\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (np.prod(input_shape),)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = input.flatten()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at flatten layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at flatten layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Layer\n",
    "Softmax Derivative: $\\frac{dS_i}{dX_j} = S_i(1 - S_i)$ if $i = j$ and $-S_iS_j$ otherwise\n",
    "\n",
    "In matrix form, obtain $C = softmax(x) \\times softmax(x)^T$ and $D = diag(softmax(x))$ where $D$ is a diagonal matrix with the softmax values on the diagonal and 0 elsewhere.\n",
    "\n",
    "$K = D - C$\n",
    "Then, $\\frac{dS}{dX} = K \\times \\frac{dE}{dO}$ where $O$ is the output of the softmax layer and $E$ is the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 1D\n",
    "        assert len(input_shape) == 1, \"Input shape must be 1D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        return np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = np.exp(input) / np.sum(np.exp(input))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at softmax layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at softmax layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        inp_softmax = self.softmax(input)\n",
    "        softmax_matrix = np.reshape(inp_softmax, (inp_softmax.shape[0], 1)) @ np.reshape(inp_softmax, (1, inp_softmax.shape[0]))\n",
    "        softmax_matrix = -1 * softmax_matrix\n",
    "        np.fill_diagonal(softmax_matrix, inp_softmax * (1 - inp_softmax))\n",
    "        input_grads = softmax_matrix @ gradWRTMyOutput\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss Layer\n",
    "\n",
    "$\\text{Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\log(p_i) + (1 - y_i) \\cdot \\log(1 - p_i) \\right]$\n",
    "\n",
    "\n",
    "$\\frac{\\partial \\text{Loss}}{\\partial p_i} = -\\frac{1}{N} \\left( \\frac{y_i}{p_i} - \\frac{1 - y_i}{1 - p_i} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 1D, typically the output of a softmax layer for two classes\n",
    "        assert len(input_shape) == 1, \"Input shape must be 1D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (1,)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Ensuring input and target shapes are correct\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        assert target.shape == self.input_shape, \"Target shape is not correct. Expected: \" + str(self.input_shape) + \" Actual: \" + str(target.shape)\n",
    "\n",
    "        # Calculating cross entropy loss\n",
    "        # Adding a small value to prevent log(0)\n",
    "        eps = 1e-15\n",
    "        self.output = -np.mean(target * np.log(input + eps) + (1 - target) * np.log(1 - input + eps))\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, target, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        # Ensuring input and target shapes are correct\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at loss layer. Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        assert target.shape == self.input_shape, \"Target shape is not correct at loss layer. Expected: \" + str(self.input_shape) + \" Actual: \" + str(target.shape)\n",
    "\n",
    "        # Calculating gradient\n",
    "        # Adding a small value to prevent division by zero\n",
    "        eps = 1e-15\n",
    "        return -(target / (input + eps) - (1 - target) / (1 - input + eps)) / input.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Layer (Helper Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer:\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. \" + \"Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = input.reshape(self.output_shape)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at reshape layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct at reshape layer.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCorrect(prediction, target):\n",
    "    return np.argmax(prediction) == np.argmax(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APAACzBVBJJwAO9dnp/wm8damu6Dw5dRjGf9IKw/+hkVPffCnWNJa7XVNV0Kxa1hErrNe/M2cnYqgElsAHpjkc1wlAODkV694W8c654t8M6n4TuvEctrrFw0cun3c0/lq+3AMJcDK5AyOeTkd+fPvGFn4gsvEtzF4m89tUG1ZJJjuMgUBVYN/EMKOe9YVXtK0bUtdvVs9LsZ7y4YgbIULYycZPoPc8V6lpfwh0/w7p66z8RdXj0y2z8llC4aWQ+mRn8lz9RXPfE3x1pvi46TYaPZTQadpMJghluWDSyrhQM9SMBe5Oc5NcBV7Tda1XRZJJNK1O8sXkG12tZ2iLD0JUjNQ3l9eahN517dT3MvTfNIXb16n6mq9Ff/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4AWNgGMyAWUhIqK5jvdSy/9/rQe5kgTlWjs3KRiAYxHsyKfDzxYMgFiOIAALDvfwQBsO/pK8Mz97fhPLAlNDtvyBwbNv3j8jCUHbAnOy/f89yM2jPwiLJwMc4628UqgQTnPvp/0eGFAQXLg5lcO/764YuhuArf3y4IAfmfoQwlBX44e/fckkMYaiA7q6/f6dJ45IViP3zdzcuSQaGn39/OkBl4WEL4euFmLIwXDuETav6lKfAIPy1DYucRNFdUPCe9MOUE3e6CpI6FogZSEKrwbFyOIATQ5v5mkcgXV9auVGlwK4NDGRguL75b88HVDla8QBFF16ADQA8sQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Display first image\n",
    "Image.fromarray(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Reshape the data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # (height, width, channel)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) # (height, width, channel)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(0)\n",
    "indices = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.9044925042331073\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = [\n",
    "    CNNLayer(kernel_size=3, number_of_kernels=2, input_shape=(28, 28, 1)),\n",
    "    SigmoidLayer(input_shape=(26, 26, 2)),\n",
    "    AveragePoolLayer(input_shape=(26, 26, 2), pool_size=2, stride=2),\n",
    "    FlattenLayer(input_shape=(13, 13, 2)),\n",
    "    ReshapeLayer(input_shape=(338,), output_shape=(1,1,338)),\n",
    "    CNNLayer(kernel_size=1, number_of_kernels=10, input_shape=(1,1,338)),\n",
    "    ReshapeLayer(input_shape=(1,1,10), output_shape=(10,)),\n",
    "    SoftmaxLayer(input_shape=(10,))\n",
    "]\n",
    "\n",
    "loss_layer = CrossEntropyLoss(input_shape=(10,))\n",
    "\n",
    "# Forward pass\n",
    "x = x_train[1]\n",
    "y = y_train[1]\n",
    "x = model[0].forward(x)\n",
    "x = model[1].forward(x)\n",
    "x = model[2].forward(x)\n",
    "x = model[3].forward(x)\n",
    "x = model[4].forward(x)\n",
    "x = model[5].forward(x)\n",
    "x = model[6].forward(x)\n",
    "x = model[7].forward(x)\n",
    "loss = loss_layer.forward(x, y)\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0, Loss: 2.588744263314928, Accuracy: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 0.07601143574590724, Accuracy: 0.14\n",
      "Epoch: 0, Iteration: 200, Loss: 0.5564269202397553, Accuracy: 0.13\n",
      "Epoch: 0, Iteration: 300, Loss: 0.8250777199999411, Accuracy: 0.15\n",
      "Epoch: 0, Iteration: 400, Loss: 0.30033168054756704, Accuracy: 0.19\n",
      "Epoch: 0, Iteration: 500, Loss: 0.2655829394801745, Accuracy: 0.1\n",
      "Epoch: 0, Iteration: 600, Loss: 0.3069032833647564, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 700, Loss: 0.5411470470429335, Accuracy: 0.2\n",
      "Epoch: 0, Iteration: 800, Loss: 0.12997636776698035, Accuracy: 0.29\n",
      "Epoch: 0, Iteration: 900, Loss: 0.5426570530837228, Accuracy: 0.1\n",
      "Epoch: 0, Iteration: 1000, Loss: 0.26815550368887825, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 1100, Loss: 0.2648740111711352, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 1200, Loss: 0.12339795744686341, Accuracy: 0.27\n",
      "Epoch: 0, Iteration: 1300, Loss: 0.3772681138534869, Accuracy: 0.22\n",
      "Epoch: 0, Iteration: 1400, Loss: 0.06410675230751153, Accuracy: 0.2\n",
      "Epoch: 0, Iteration: 1500, Loss: 0.17511056785547724, Accuracy: 0.18\n",
      "Epoch: 0, Iteration: 1600, Loss: 0.29590052095143043, Accuracy: 0.21\n",
      "Epoch: 0, Iteration: 1700, Loss: 0.4867199266696189, Accuracy: 0.33\n",
      "Epoch: 0, Iteration: 1800, Loss: 0.5983066037757945, Accuracy: 0.3\n",
      "Epoch: 0, Iteration: 1900, Loss: 0.3009227696112952, Accuracy: 0.27\n",
      "Epoch: 0, Iteration: 2000, Loss: 0.024388583279080975, Accuracy: 0.33\n",
      "Epoch: 0, Iteration: 2100, Loss: 0.1436397601311616, Accuracy: 0.31\n",
      "Epoch: 0, Iteration: 2200, Loss: 0.08916829101679925, Accuracy: 0.45\n",
      "Epoch: 0, Iteration: 2300, Loss: 0.10949858022953836, Accuracy: 0.46\n",
      "Epoch: 0, Iteration: 2400, Loss: 0.29504737028125105, Accuracy: 0.43\n",
      "Epoch: 0, Iteration: 2500, Loss: 0.029426333179572384, Accuracy: 0.35\n",
      "Epoch: 0, Iteration: 2600, Loss: 0.14582994359314821, Accuracy: 0.39\n",
      "Epoch: 0, Iteration: 2700, Loss: 0.35521184141861617, Accuracy: 0.5\n",
      "Epoch: 0, Iteration: 2800, Loss: 0.41389604711425887, Accuracy: 0.41\n",
      "Epoch: 0, Iteration: 2900, Loss: 0.047030651146234, Accuracy: 0.51\n",
      "Epoch: 0, Iteration: 3000, Loss: 0.4023842421658436, Accuracy: 0.46\n",
      "Epoch: 0, Iteration: 3100, Loss: 0.05444547809185183, Accuracy: 0.5\n",
      "Epoch: 0, Iteration: 3200, Loss: 0.10206562710867453, Accuracy: 0.49\n",
      "Epoch: 0, Iteration: 3300, Loss: 0.5140524275740147, Accuracy: 0.53\n",
      "Epoch: 0, Iteration: 3400, Loss: 0.20813280948521767, Accuracy: 0.52\n",
      "Epoch: 0, Iteration: 3500, Loss: 0.2196527780817512, Accuracy: 0.49\n",
      "Epoch: 0, Iteration: 3600, Loss: 0.46140957440885827, Accuracy: 0.42\n",
      "Epoch: 0, Iteration: 3700, Loss: 0.15013380690060973, Accuracy: 0.55\n",
      "Epoch: 0, Iteration: 3800, Loss: 0.045001991227222246, Accuracy: 0.5\n",
      "Epoch: 0, Iteration: 3900, Loss: 0.0028830659792345144, Accuracy: 0.55\n",
      "Epoch: 0, Iteration: 4000, Loss: 0.12313644423264021, Accuracy: 0.62\n",
      "Epoch: 0, Iteration: 4100, Loss: 0.1242127754226893, Accuracy: 0.6\n",
      "Epoch: 0, Iteration: 4200, Loss: 0.5398319165769977, Accuracy: 0.55\n",
      "Epoch: 0, Iteration: 4300, Loss: 0.25283747670823253, Accuracy: 0.63\n",
      "Epoch: 0, Iteration: 4400, Loss: 0.16270593412913428, Accuracy: 0.57\n",
      "Epoch: 0, Iteration: 4500, Loss: 0.1319682645368723, Accuracy: 0.61\n",
      "Epoch: 0, Iteration: 4600, Loss: 0.014733879955084305, Accuracy: 0.58\n",
      "Epoch: 0, Iteration: 4700, Loss: 0.07245870687644568, Accuracy: 0.59\n",
      "Epoch: 0, Iteration: 4800, Loss: 0.010232971839168417, Accuracy: 0.62\n",
      "Epoch: 0, Iteration: 4900, Loss: 0.4549251391240035, Accuracy: 0.6\n",
      "Epoch: 0, Iteration: 5000, Loss: 0.307011619512354, Accuracy: 0.59\n",
      "Epoch: 0, Iteration: 5100, Loss: 0.13250003243040584, Accuracy: 0.63\n",
      "Epoch: 0, Iteration: 5200, Loss: 0.08412114369802554, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 5300, Loss: 0.01664546813943734, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 5400, Loss: 0.5415958168641438, Accuracy: 0.62\n",
      "Epoch: 0, Iteration: 5500, Loss: 0.027651314532594385, Accuracy: 0.68\n",
      "Epoch: 0, Iteration: 5600, Loss: 0.04162804986715408, Accuracy: 0.65\n",
      "Epoch: 0, Iteration: 5700, Loss: 0.1175339099384454, Accuracy: 0.68\n",
      "Epoch: 0, Iteration: 5800, Loss: 0.18215962296489419, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 5900, Loss: 0.2503621509815501, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 6000, Loss: 0.029762703096626875, Accuracy: 0.65\n",
      "Epoch: 0, Iteration: 6100, Loss: 0.16982923855234855, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 6200, Loss: 0.38342161481305914, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 6300, Loss: 0.1013886569131176, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 6400, Loss: 0.08950821144339363, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 6500, Loss: 0.15092839398102328, Accuracy: 0.72\n",
      "Epoch: 0, Iteration: 6600, Loss: 0.05265972387718078, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 6700, Loss: 0.04339252456359994, Accuracy: 0.65\n",
      "Epoch: 0, Iteration: 6800, Loss: 0.20776395865867156, Accuracy: 0.72\n",
      "Epoch: 0, Iteration: 6900, Loss: 0.018703639613170713, Accuracy: 0.63\n",
      "Epoch: 0, Iteration: 7000, Loss: 0.03147596204114097, Accuracy: 0.73\n",
      "Epoch: 0, Iteration: 7100, Loss: 0.02575441951196572, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 7200, Loss: 0.01362015242739533, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 7300, Loss: 0.033210303648370146, Accuracy: 0.72\n",
      "Epoch: 0, Iteration: 7400, Loss: 0.027924392410199672, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 7500, Loss: 0.5639757512754192, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 7600, Loss: 0.1549240959512333, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 7700, Loss: 0.028593136070905224, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 7800, Loss: 0.08183683667094699, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 7900, Loss: 0.008394549728061721, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 8000, Loss: 0.26821092804175023, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 8100, Loss: 0.0002158690363585518, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 8200, Loss: 0.011992319885477071, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 8300, Loss: 0.022170984957747712, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 8400, Loss: 0.08205003834672286, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 8500, Loss: 0.1162641986062752, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 8600, Loss: 0.08440277370947472, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 8700, Loss: 0.2780856664127792, Accuracy: 0.69\n",
      "Epoch: 0, Iteration: 8800, Loss: 0.004232260873030207, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 8900, Loss: 0.031275813489588564, Accuracy: 0.67\n",
      "Epoch: 0, Iteration: 9000, Loss: 0.09387785663301312, Accuracy: 0.66\n",
      "Epoch: 0, Iteration: 9100, Loss: 0.0063798122554921234, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 9200, Loss: 0.16562092289072933, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 9300, Loss: 0.0004214918267828866, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 9400, Loss: 0.010279637439512813, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 9500, Loss: 0.14379373877915946, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 9600, Loss: 0.054192736786335315, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 9700, Loss: 0.21208856024083816, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 9800, Loss: 0.016888158557844497, Accuracy: 0.73\n",
      "Epoch: 0, Iteration: 9900, Loss: 0.09897843684592357, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 10000, Loss: 0.008120423368834544, Accuracy: 0.7\n",
      "Epoch: 0, Iteration: 10100, Loss: 0.013060419063406775, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 10200, Loss: 0.09027454664529842, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 10300, Loss: 0.04783156271947832, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 10400, Loss: 0.19113661753746175, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 10500, Loss: 0.03913178887180474, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 10600, Loss: 0.22466334527981885, Accuracy: 0.71\n",
      "Epoch: 0, Iteration: 10700, Loss: 0.7481527760775004, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 10800, Loss: 0.006440869733941162, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 10900, Loss: 0.01507373592838423, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 11000, Loss: 0.027156581447572964, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 11100, Loss: 0.00045318880032862286, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 11200, Loss: 0.006794328216217217, Accuracy: 0.73\n",
      "Epoch: 0, Iteration: 11300, Loss: 0.012447820986557234, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 11400, Loss: 0.04650713884573444, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 11500, Loss: 0.021470614748931544, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 11600, Loss: 0.004565791664037309, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 11700, Loss: 0.0031649998510550866, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 11800, Loss: 0.030519065519469535, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 11900, Loss: 0.013019587523114839, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 12000, Loss: 0.009618124225356139, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 12100, Loss: 0.00034188543358516484, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 12200, Loss: 1.871753264509414, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 12300, Loss: 0.5722772016621225, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 12400, Loss: 0.007413818629040864, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 12500, Loss: 0.43455674540613565, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 12600, Loss: 0.6212445764058565, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 12700, Loss: 0.21407668526098353, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 12800, Loss: 0.006875307485997829, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 12900, Loss: 0.037915863675983805, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 13000, Loss: 0.008246211724610309, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 13100, Loss: 0.003145425108320516, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 13200, Loss: 1.4925570952373453, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 13300, Loss: 0.20354620957571923, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 13400, Loss: 0.03946266375296081, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 13500, Loss: 0.49643358355952527, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 13600, Loss: 0.27254972170960423, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 13700, Loss: 0.023019979509446873, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 13800, Loss: 0.12720208935779168, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 13900, Loss: 0.005177963944727823, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 14000, Loss: 0.01961485748874597, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 14100, Loss: 0.025388521579145524, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 14200, Loss: 0.005399641056577625, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 14300, Loss: 0.05432051314837255, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 14400, Loss: 0.32318282536861775, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 14500, Loss: 0.022620567195470353, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 14600, Loss: 0.4686686869015079, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 14700, Loss: 0.06291853346239809, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 14800, Loss: 0.033865303578867585, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 14900, Loss: 0.025354738809312004, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 15000, Loss: 0.0018052479203970405, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 15100, Loss: 0.02379510794470261, Accuracy: 0.74\n",
      "Epoch: 0, Iteration: 15200, Loss: 0.024880390556363426, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 15300, Loss: 0.09449597410745204, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 15400, Loss: 0.26018540899630205, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 15500, Loss: 0.30192729242205396, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 15600, Loss: 0.013571658609225457, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 15700, Loss: 0.007106708726420966, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 15800, Loss: 0.44835823267823854, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 15900, Loss: 0.12505826990348773, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 16000, Loss: 0.02290600175244382, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16100, Loss: 0.18064747758033903, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 16200, Loss: 0.05207944739161123, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 16300, Loss: 0.08244450266408632, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 16400, Loss: 0.003073588511427137, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16500, Loss: 0.1103458343267997, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16600, Loss: 0.0018717701686342184, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 16700, Loss: 0.020309664207114837, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 16800, Loss: 0.048962392674177395, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 16900, Loss: 0.34869283518371674, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 17000, Loss: 0.07132632045262362, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 17100, Loss: 0.026241207199946992, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 17200, Loss: 0.11144788578576388, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 17300, Loss: 0.0800297821548414, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 17400, Loss: 0.003822815956596092, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 17500, Loss: 0.30891342201664096, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 17600, Loss: 0.00020972869860564226, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 17700, Loss: 0.04242977267018721, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 17800, Loss: 0.0034061484735977967, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 17900, Loss: 0.059849275571717084, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 18000, Loss: 0.15771756602797704, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 18100, Loss: 0.002973452036033439, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 18200, Loss: 0.01789582080003587, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 18300, Loss: 0.006252356376377216, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 18400, Loss: 0.004463822282200497, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 18500, Loss: 0.00040655892714818777, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 18600, Loss: 0.13061807112725282, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 18700, Loss: 0.007221046063113494, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 18800, Loss: 0.008713091302044182, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 18900, Loss: 0.0669692732886448, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 19000, Loss: 0.34796338491917156, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 19100, Loss: 0.0006307807206212589, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 19200, Loss: 0.006168351882795873, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 19300, Loss: 0.08516675370978917, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 19400, Loss: 0.0060983453624798775, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 19500, Loss: 0.00017602650723704541, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 19600, Loss: 0.016444472878691923, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 19700, Loss: 0.03630590981184442, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 19800, Loss: 0.11088374000996204, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 19900, Loss: 0.008894514276621685, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 20000, Loss: 0.02254982031478193, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 20100, Loss: 0.010925106155848362, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 20200, Loss: 0.13963707130700145, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 20300, Loss: 0.006479112112187856, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 20400, Loss: 0.0001580461564391022, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 20500, Loss: 0.012017371554769049, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 20600, Loss: 0.04292485720839007, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 20700, Loss: 0.00018819475632065653, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 20800, Loss: 0.0001339988935164298, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 20900, Loss: 0.002923114841774856, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 21000, Loss: 0.004926783933278322, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 21100, Loss: 0.010440440989937566, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 21200, Loss: 0.01859424509428121, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 21300, Loss: 0.0004151249922383373, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 21400, Loss: 0.000858785293247059, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 21500, Loss: 0.0002718489741356237, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 21600, Loss: 0.024749856278620013, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 21700, Loss: 0.21061309325639904, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 21800, Loss: 0.01765966189982983, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 21900, Loss: 0.4724848356903103, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 22000, Loss: 0.0020360435917123605, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 22100, Loss: 0.14436426796794466, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 22200, Loss: 0.05372524842369382, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 22300, Loss: 0.06905824304635756, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 22400, Loss: 0.030363442701283584, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 22500, Loss: 0.06113620759094168, Accuracy: 0.75\n",
      "Epoch: 0, Iteration: 22600, Loss: 0.005687315777684072, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 22700, Loss: 0.014869257236705031, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 22800, Loss: 0.16684679286203613, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 22900, Loss: 0.16516052128233744, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 23000, Loss: 0.0009547010795013227, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 23100, Loss: 0.025922505506127864, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 23200, Loss: 0.2011940541432649, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 23300, Loss: 0.0016725930860921653, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 23400, Loss: 0.00099958791850794, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 23500, Loss: 0.0006540873920767703, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 23600, Loss: 0.0699878418058295, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 23700, Loss: 0.005178932957989297, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 23800, Loss: 0.10955474721117801, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 23900, Loss: 0.00036557167685893915, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 24000, Loss: 0.07932916839415585, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 24100, Loss: 0.001444663784985522, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 24200, Loss: 0.0007036622557004808, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 24300, Loss: 0.06102945479374021, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 24400, Loss: 0.00967960120316795, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 24500, Loss: 0.00037697899517921473, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 24600, Loss: 0.011679038234632897, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 24700, Loss: 0.00601282329224486, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 24800, Loss: 0.0013339654977338965, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 24900, Loss: 0.00048301138602439, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 25000, Loss: 0.004664331784149279, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 25100, Loss: 0.00011068060316668786, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 25200, Loss: 0.09635046904805891, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 25300, Loss: 0.03913965495361183, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 25400, Loss: 0.01615379515888552, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 25500, Loss: 0.036350616578606165, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 25600, Loss: 0.00015140284996500685, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 25700, Loss: 0.002373228817498788, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 25800, Loss: 0.033254943981939414, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 25900, Loss: 0.00012578446543857, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 26000, Loss: 0.03952871603057413, Accuracy: 0.94\n",
      "Epoch: 0, Iteration: 26100, Loss: 0.015021213737783595, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 26200, Loss: 0.000759689710177732, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 26300, Loss: 0.6279295389553374, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 26400, Loss: 0.0019416068828550692, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 26500, Loss: 8.19140335879717e-05, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 26600, Loss: 0.0002639793713065424, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 26700, Loss: 0.020514219947689177, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 26800, Loss: 0.002985388610046559, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 26900, Loss: 0.001937987152966523, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 27000, Loss: 0.004439794527998437, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 27100, Loss: 0.007071732485285645, Accuracy: 0.76\n",
      "Epoch: 0, Iteration: 27200, Loss: 0.001570234527739117, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 27300, Loss: 0.004842248986204521, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 27400, Loss: 0.13949278722468367, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 27500, Loss: 0.0560892185845059, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 27600, Loss: 0.001801092139144021, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 27700, Loss: 0.0032655128107318618, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 27800, Loss: 0.13088051399860876, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 27900, Loss: 0.026930762735155434, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 28000, Loss: 0.0033506911385877782, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 28100, Loss: 0.006383090153543341, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 28200, Loss: 0.0360411583152885, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 28300, Loss: 0.004346419084294552, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 28400, Loss: 6.672326965676357e-05, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 28500, Loss: 0.0028389510742481134, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 28600, Loss: 0.006875813745685825, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 28700, Loss: 0.058234839595771845, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 28800, Loss: 0.9681324801145278, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 28900, Loss: 0.04180625660118084, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 29000, Loss: 0.005888442429651062, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 29100, Loss: 0.0035973552579801546, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 29200, Loss: 0.10813018244414492, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 29300, Loss: 0.004311594854319021, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 29400, Loss: 0.07723032548125017, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 29500, Loss: 0.1756912156033128, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 29600, Loss: 0.01876457335689876, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 29700, Loss: 0.009103450891194388, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 29800, Loss: 0.019038992668331606, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 29900, Loss: 0.002154905262202934, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 30000, Loss: 0.30651701858047675, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 30100, Loss: 0.00033252328675779637, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 30200, Loss: 0.0037932427095640894, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 30300, Loss: 0.048259588503032046, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 30400, Loss: 0.0007788277040502022, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 30500, Loss: 0.004436663671120099, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 30600, Loss: 0.005528047052216678, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 30700, Loss: 0.021319035680180377, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 30800, Loss: 0.0018852889060485406, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 30900, Loss: 0.04733001864271596, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 31000, Loss: 0.161611543127447, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 31100, Loss: 0.000498723489492423, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 31200, Loss: 0.0017346772393830282, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 31300, Loss: 0.6029372464158057, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 31400, Loss: 0.0009963238006082487, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 31500, Loss: 0.0002926743037362467, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 31600, Loss: 0.019408194595146165, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 31700, Loss: 0.026641254204533015, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 31800, Loss: 0.007932894059491033, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 31900, Loss: 0.014624223408655082, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 32000, Loss: 0.3668712485867548, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 32100, Loss: 0.5903099960432241, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 32200, Loss: 0.007357382508134044, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 32300, Loss: 0.24240338386982732, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 32400, Loss: 0.0012884759808616776, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 32500, Loss: 0.00016906963151765945, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 32600, Loss: 9.713602231031951e-05, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 32700, Loss: 0.012153587670717681, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 32800, Loss: 0.000570070918467638, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 32900, Loss: 0.22237878559723095, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 33000, Loss: 0.0004641456599109809, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 33100, Loss: 0.004951456322195427, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 33200, Loss: 8.512983287021513e-05, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 33300, Loss: 0.0008596242281246854, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 33400, Loss: 0.00537964446211986, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 33500, Loss: 0.14962163584538163, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 33600, Loss: 0.015519440012921156, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 33700, Loss: 0.022129660111736026, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 33800, Loss: 0.40689110267425177, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 33900, Loss: 0.007903144831373294, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 34000, Loss: 0.00823704214888514, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 34100, Loss: 0.003463912345652846, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 34200, Loss: 0.00022245330703324494, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 34300, Loss: 0.005848042596093727, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 34400, Loss: 0.0986105315022108, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 34500, Loss: 0.04766968988819344, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 34600, Loss: 0.06646118266175066, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 34700, Loss: 0.0017304110944951167, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 34800, Loss: 0.13329718553366254, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 34900, Loss: 0.014671132153771974, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 35000, Loss: 6.297686014312872e-05, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 35100, Loss: 0.010791914679018135, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 35200, Loss: 0.0016331736534783952, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35300, Loss: 0.012759086452671908, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 35400, Loss: 0.04456374071588643, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35500, Loss: 0.0009371946776959054, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35600, Loss: 0.0003096988495626332, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 35700, Loss: 0.00546060857992511, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 35800, Loss: 0.00583217492127691, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 35900, Loss: 0.6203108026495475, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 36000, Loss: 0.028886904460079264, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 36100, Loss: 0.004697376937527182, Accuracy: 0.94\n",
      "Epoch: 0, Iteration: 36200, Loss: 0.17018699365829298, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 36300, Loss: 0.07173446270590693, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 36400, Loss: 0.041160578058632866, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 36500, Loss: 0.00019499082187151787, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 36600, Loss: 0.5317793985658558, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 36700, Loss: 0.004933963577637665, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 36800, Loss: 0.047900654738107035, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 36900, Loss: 0.2593535757200941, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 37000, Loss: 0.00026643943275869714, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 37100, Loss: 0.04101764356126339, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 37200, Loss: 0.0018505606341249364, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 37300, Loss: 0.0004961132157607531, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 37400, Loss: 0.07237410788962152, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 37500, Loss: 0.0009144819344885424, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 37600, Loss: 0.5484392345577994, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 37700, Loss: 0.0020269070839818476, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 37800, Loss: 0.007912694156841335, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 37900, Loss: 4.244796141411557e-06, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 38000, Loss: 0.015160828995373188, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 38100, Loss: 0.039262147546530304, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 38200, Loss: 0.0048532965243104804, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 38300, Loss: 0.021840812730844168, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 38400, Loss: 0.01486502759282537, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 38500, Loss: 0.0007585615091490598, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 38600, Loss: 0.0033166055394646204, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 38700, Loss: 0.18645274482185353, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 38800, Loss: 0.010145828509793332, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 38900, Loss: 0.10949238815570297, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 39000, Loss: 0.0039966304759349956, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 39100, Loss: 0.04088861777627041, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 39200, Loss: 0.031489454818251585, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 39300, Loss: 0.0019434306884944439, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 39400, Loss: 0.06375364736331243, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 39500, Loss: 0.27647275252126874, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 39600, Loss: 0.0029489226019530513, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 39700, Loss: 0.0011437585455171856, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 39800, Loss: 0.0008929687468553421, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 39900, Loss: 0.3444166708865425, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 40000, Loss: 0.509652946546748, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 40100, Loss: 0.055885926034810976, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 40200, Loss: 0.0007489274063751253, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 40300, Loss: 0.024805045361794304, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 40400, Loss: 0.016282210282778257, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 40500, Loss: 0.03574264399355574, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 40600, Loss: 0.0013169894260492362, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 40700, Loss: 0.00755932375814401, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 40800, Loss: 0.3177714474672682, Accuracy: 0.77\n",
      "Epoch: 0, Iteration: 40900, Loss: 0.028949504003470482, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 41000, Loss: 0.000389110786952103, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 41100, Loss: 0.014972795698833896, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 41200, Loss: 2.5392378021529752e-05, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 41300, Loss: 0.5275524317216377, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 41400, Loss: 0.023717621052152197, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 41500, Loss: 0.0034771585377887408, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 41600, Loss: 0.029305559002466668, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 41700, Loss: 0.14022514686467108, Accuracy: 0.92\n",
      "Epoch: 0, Iteration: 41800, Loss: 0.004841835238440647, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 41900, Loss: 0.029320141168847957, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 42000, Loss: 0.00016548394154475662, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 42100, Loss: 0.003979780505688483, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 42200, Loss: 0.040687999221595175, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 42300, Loss: 0.0026378344178080654, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 42400, Loss: 0.0006266381897515941, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 42500, Loss: 0.007376376803056151, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 42600, Loss: 0.023265124501565787, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 42700, Loss: 0.055183486292299036, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 42800, Loss: 0.13910947501969576, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 42900, Loss: 0.2557011488287074, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 43000, Loss: 0.00045276121624860267, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 43100, Loss: 0.002721416343515579, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 43200, Loss: 0.009525908400169704, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 43300, Loss: 0.43661848266618897, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 43400, Loss: 0.07917627341158354, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 43500, Loss: 0.0026456149191752608, Accuracy: 0.78\n",
      "Epoch: 0, Iteration: 43600, Loss: 0.04110527073139861, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 43700, Loss: 0.008969728365522046, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 43800, Loss: 0.08612181582033651, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 43900, Loss: 0.42319405446575287, Accuracy: 0.81\n",
      "Epoch: 0, Iteration: 44000, Loss: 2.1748797204870195e-05, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 44100, Loss: 0.02057867504240792, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 44200, Loss: 0.000663461831479001, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 44300, Loss: 0.008589039373103659, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 44400, Loss: 0.015528009088043835, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 44500, Loss: 0.04943005089037418, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 44600, Loss: 0.02792873330215554, Accuracy: 0.82\n",
      "Epoch: 0, Iteration: 44700, Loss: 0.12722762893310285, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 44800, Loss: 0.0001389614838168361, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 44900, Loss: 0.08814567102866334, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 45000, Loss: 0.10756960836563778, Accuracy: 0.8\n",
      "Epoch: 0, Iteration: 45100, Loss: 0.4798681320437222, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 45200, Loss: 0.000286375272873295, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 45300, Loss: 0.19545823192128572, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 45400, Loss: 0.0012688990257121887, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 45500, Loss: 0.0319781434720514, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 45600, Loss: 0.00854449441090977, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 45700, Loss: 0.056209676434384306, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 45800, Loss: 0.0003590982264220987, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 45900, Loss: 0.010258421303887618, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 46000, Loss: 0.009819790668422628, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 46100, Loss: 0.002020244086095535, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 46200, Loss: 0.004618354861910238, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 46300, Loss: 0.19731010150278788, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 46400, Loss: 0.0012550866010851373, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 46500, Loss: 0.0030601535527749434, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 46600, Loss: 0.1355931198409232, Accuracy: 0.91\n",
      "Epoch: 0, Iteration: 46700, Loss: 0.0014637085940231473, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 46800, Loss: 0.028548462077657773, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 46900, Loss: 0.00011038997056432638, Accuracy: 0.87\n",
      "Epoch: 0, Iteration: 47000, Loss: 0.09764683240637295, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 47100, Loss: 0.0011250546229216588, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 47200, Loss: 0.0011497203976501588, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 47300, Loss: 0.04889997279464144, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 47400, Loss: 0.004967814828649937, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 47500, Loss: 0.0015544976277850305, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 47600, Loss: 0.004608996464633909, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 47700, Loss: 0.30967320437447887, Accuracy: 0.84\n",
      "Epoch: 0, Iteration: 47800, Loss: 0.0012748225034455458, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 47900, Loss: 0.12058933170008383, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 48000, Loss: 5.9125456201806556e-05, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 48100, Loss: 0.04096805928747256, Accuracy: 0.88\n",
      "Epoch: 0, Iteration: 48200, Loss: 1.1208253302649036, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 48300, Loss: 0.004678684853098509, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 48400, Loss: 0.0011150197515747394, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 48500, Loss: 0.0002649223372293236, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 48600, Loss: 0.0006054342910541766, Accuracy: 0.93\n",
      "Epoch: 0, Iteration: 48700, Loss: 0.0007942031143003002, Accuracy: 0.95\n",
      "Epoch: 0, Iteration: 48800, Loss: 0.00983118969051489, Accuracy: 0.79\n",
      "Epoch: 0, Iteration: 48900, Loss: 0.0037004592344059826, Accuracy: 0.83\n",
      "Epoch: 0, Iteration: 49000, Loss: 0.0020654221798754075, Accuracy: 0.89\n",
      "Epoch: 0, Iteration: 49100, Loss: 0.29203749167312926, Accuracy: 0.86\n",
      "Epoch: 0, Iteration: 49200, Loss: 0.0002534313080154375, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 49300, Loss: 0.007497517103291761, Accuracy: 0.9\n",
      "Epoch: 0, Iteration: 49400, Loss: 0.0001751552038284522, Accuracy: 0.85\n",
      "Epoch: 0, Iteration: 49500, Loss: 0.004789557847884323, Accuracy: 0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m grad \u001b[39m=\u001b[39m model[\u001b[39m7\u001b[39m]\u001b[39m.\u001b[39mbackward(layer6, grad, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m grad \u001b[39m=\u001b[39m model[\u001b[39m6\u001b[39m]\u001b[39m.\u001b[39mbackward(layer5, grad, learning_rate)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m grad \u001b[39m=\u001b[39m model[\u001b[39m5\u001b[39;49m]\u001b[39m.\u001b[39;49mbackward(layer4, grad, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m grad \u001b[39m=\u001b[39m model[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mbackward(layer3, grad, learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m grad \u001b[39m=\u001b[39m model[\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mbackward(layer2, grad, learning_rate)\n",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_channels):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumber_of_kernels):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m         input_grads[:, :, j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mconvolve2d(gradWRTMyOutput[:, :, i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernels[:, :, j, i], mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# Update kernels and biases\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/CNN_Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernels \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m kernel_grads\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train \n",
    "epochs = 5\n",
    "learning_rate = 0.1\n",
    "gamma = 0.5\n",
    "interval = 1\n",
    "for epoch in range(epochs):\n",
    "    if epoch > 0 and epoch % interval == 0:\n",
    "        learning_rate = learning_rate * gamma\n",
    "    train_correct = 0\n",
    "    for i in range(len(x_train)):\n",
    "        x = x_train[i]\n",
    "        y = y_train[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        layer0 = model[0].forward(x) # CNN with kernel size 3\n",
    "        layer1 = model[1].forward(layer0) # Sigmoid\n",
    "        layer2 = model[2].forward(layer1) # Average pool\n",
    "        layer3 = model[3].forward(layer2) # Flatten\n",
    "        layer4 = model[4].forward(layer3) # Reshape\n",
    "        layer5 = model[5].forward(layer4) # CNN with kernel size 1\n",
    "        layer6 = model[6].forward(layer5) # Reshape\n",
    "        layer7 = model[7].forward(layer6) # Softmax\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_layer.forward(layer7, y) # Cross entropy\n",
    "        # accuracy\n",
    "        train_correct += isCorrect(layer7, y)\n",
    "        \n",
    "        # Grad\n",
    "        grad = loss_layer.backward(layer7, y, learning_rate) # Cross entropy\n",
    "        grad = model[7].backward(layer6, grad, learning_rate) # Softmax\n",
    "        grad = model[6].backward(layer5, grad, learning_rate) # Reshape\n",
    "        grad = model[5].backward(layer4, grad, learning_rate) # CNN with kernel size 1\n",
    "        grad = model[4].backward(layer3, grad, learning_rate) # Reshape\n",
    "        grad = model[3].backward(layer2, grad, learning_rate) # Flatten\n",
    "        grad = model[2].backward(layer1, grad, learning_rate) # Average pool\n",
    "        grad = model[1].backward(layer0, grad, learning_rate) # Sigmoid\n",
    "        grad = model[0].backward(x, grad, learning_rate) # CNN with kernel size 3\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch: \" + str(epoch) + \", Iteration: \" + str(i) + \", Loss: \" + str(loss) + \", Accuracy: \" + str(train_correct / 100))\n",
    "            train_correct = 0\n",
    "            \n",
    "    # Test\n",
    "    test_correct = 0\n",
    "    for i in range(len(x_test)):\n",
    "        x = x_test[i]\n",
    "        y = y_test[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        layer0 = model[0].forward(x) # CNN with kernel size 3\n",
    "        layer1 = model[1].forward(layer0) # Sigmoid\n",
    "        layer2 = model[2].forward(layer1) # Average pool\n",
    "        layer3 = model[3].forward(layer2) # Flatten\n",
    "        layer4 = model[4].forward(layer3) # Reshape\n",
    "        layer5 = model[5].forward(layer4) # CNN with kernel size 1\n",
    "        layer6 = model[6].forward(layer5) # Reshape\n",
    "        layer7 = model[7].forward(layer6) # Softmax\n",
    "        \n",
    "        # accuracy\n",
    "        test_correct += isCorrect(layer7, y)\n",
    "    \n",
    "    print(\"Epoch: \" + str(epoch) + \", Test Accuracy: \" + str(test_correct / len(x_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonus-project-vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
