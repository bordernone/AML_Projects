{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer:\n",
    "    def __init__(self, kernel_size, number_of_kernels, input_shape):\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\" \n",
    "        self.input_channels = input_shape[2]\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.kernel_size = kernel_size\n",
    "        self.number_of_kernels = number_of_kernels\n",
    "        self.kernels = np.random.randn(kernel_size, kernel_size, self.input_channels, number_of_kernels)\n",
    "        \n",
    "        # output shape\n",
    "        self.output_shape = (input_shape[0] - kernel_size + 1, input_shape[1] - kernel_size + 1, number_of_kernels)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. \" + \"Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "\n",
    "        self.input = input\n",
    "        output = np.zeros(self.output_shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                imageChannel = input[:, :, j]\n",
    "                filter = self.kernels[:, :, j, i]\n",
    "                result = np.zeros((imageChannel.shape[0] - filter.shape[0] + 1, imageChannel.shape[1] - filter.shape[1] + 1))\n",
    "                for x in range(result.shape[0]):\n",
    "                    for y in range(result.shape[1]):\n",
    "                        result[x, y] = np.sum(imageChannel[x:x+filter.shape[0], y:y+filter.shape[1]] * filter)\n",
    "                output[:, :, i] += result\n",
    "        \n",
    "        output += self.biases\n",
    "        \n",
    "        self.output = output\n",
    "        return output\n",
    "    \n",
    "    def backward(self, gradWRTMyOutput, learning_rate):\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at CNN layer with kernel size: \" + str(self.kernel_size) + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        \n",
    "        input = self.input\n",
    "        # Compute gradient with respect to kernels\n",
    "        kernel_grads = np.zeros(self.kernels.shape)\n",
    "        for i in range(self.number_of_kernels):\n",
    "            for j in range(self.input_channels):\n",
    "                channel = input[:, :, j]\n",
    "                kernel = gradWRTMyOutput[:, :, i]\n",
    "                result = np.zeros((channel.shape[0] - kernel.shape[0] + 1, channel.shape[1] - kernel.shape[1] + 1))\n",
    "                for x in range(result.shape[0]):\n",
    "                    for y in range(result.shape[1]):\n",
    "                        result[x, y] = np.sum(channel[x:x+kernel.shape[0], y:y+kernel.shape[1]] * kernel)\n",
    "                kernel_grads[:, :, j, i] = result\n",
    "\n",
    "        # Compute gradient with respect to biases\n",
    "        bias_grads = gradWRTMyOutput.copy()\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        for j in range(self.input_channels):\n",
    "            for i in range(self.number_of_kernels):\n",
    "                channel = gradWRTMyOutput[:, :, i]\n",
    "                kernel = self.kernels[:, :, j, i]\n",
    "                kernel = np.flip(kernel)\n",
    "                result = np.zeros((channel.shape[0] + kernel.shape[0] - 1, channel.shape[1] + kernel.shape[1] - 1))\n",
    "                for x in range(result.shape[0]):\n",
    "                    for y in range(result.shape[1]):\n",
    "                        for k in range(kernel.shape[0]):\n",
    "                            for l in range(kernel.shape[1]):\n",
    "                                if x - k >= 0 and x - k < channel.shape[0] and y - l >= 0 and y - l < channel.shape[1]:\n",
    "                                    result[x, y] += channel[x - k, y - l] * kernel[k, l]\n",
    "                \n",
    "                input_grads[:, :, j] += result                \n",
    "        \n",
    "        # Update kernels and biases\n",
    "        self.kernels -= learning_rate * kernel_grads\n",
    "        self.biases -= learning_rate * bias_grads\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z')\n",
    "\n",
    "Error_Grad_WRT_Input: Error_Grad_WRT_Output * Output_Grad_WRT_Input , where * is element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = 1.0 / (1.0 + np.exp(-input))\n",
    "        self.input = input\n",
    "        return self.output\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1.0 - x)\n",
    "    \n",
    "    def backward(self, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at sigmoid layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        # element-wise gradient\n",
    "        gradMyOutputWRTMyInput = self.sigmoid_derivative(self.output)\n",
    "        # element-wise multiplication\n",
    "        return gradWRTMyOutput * gradMyOutputWRTMyInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Pooling Layer\n",
    "Input: (x, y, z) where z is the number of channels\n",
    "\n",
    "Output: (x', y', z)\n",
    "\n",
    "Error Gradient\n",
    "\n",
    "$\\frac{dE}{dX_{ijk}} = \\sum_{m,n} \\frac{dE}{dY_{mnk}} \\times \\frac{1}{\\text{pool\\_size}^2} \\times \\mathbb{\\theta}_{i, j}$ where $\\mathbb{\\theta}_{i, j}$ is 1 if $X_{ijk}$ was included when calculating $Y_{mnk}$ and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePoolLayer:\n",
    "    def __init__(self, input_shape, pool_size, stride):\n",
    "        # Input shape is 3D\n",
    "        assert len(input_shape) == 3, \"Input shape must be 3D\"\n",
    "        # input shape must be divisible by pool size\n",
    "        assert input_shape[0] % pool_size == 0, \"Input shape must be divisible by pool size\"\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (input_shape[0] // stride, input_shape[1] // stride, input_shape[2])\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\" + \" Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "        for i in range(self.output_shape[0]):\n",
    "            for j in range(self.output_shape[1]):\n",
    "                for k in range(self.output_shape[2]):\n",
    "                    self.output[i, j, k] = np.mean(input[i*self.stride:i*self.stride+self.pool_size, j*self.stride:j*self.stride+self.pool_size, k])\n",
    "        \n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at average pool layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        \n",
    "        input = self.input\n",
    "        \n",
    "        # Compute gradient with respect to input\n",
    "        avg_pool_n = self.pool_size * self.pool_size # number of elements involved in the average pool operation\n",
    "        input_grads = np.zeros(input.shape)\n",
    "        \n",
    "        for channel in range(self.input_shape[2]):\n",
    "            for i in range(self.output_shape[0]):\n",
    "                for j in range(self.output_shape[1]):\n",
    "                    # Only loop over the elements involved in the average pool operation\n",
    "                    for x in range(i*self.stride, i*self.stride+self.pool_size):\n",
    "                        for y in range(j*self.stride, j*self.stride+self.pool_size):\n",
    "                            input_grads[x, y, channel] += gradWRTMyOutput[i, j, channel] / avg_pool_n\n",
    "        \n",
    "        return input_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenLayer():\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = (np.prod(input_shape),)\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct\"\n",
    "        self.output = input.flatten()\n",
    "        self.input = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at flatten layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        \n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeLayer:\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct. \" + \"Expected: \" + str(self.input_shape) + \" Actual: \" + str(input.shape)\n",
    "        self.output = input.reshape(self.output_shape)\n",
    "        self.input = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradWRTMyOutput, learning_rate): # learning_rate is not used, but it is here to keep the same interface\n",
    "        assert gradWRTMyOutput.shape == self.output_shape, \"Grad shape is not correct at reshape layer.\" + \" Expected: \" + str(self.output_shape) + \" Actual: \" + str(gradWRTMyOutput.shape)\n",
    "        return gradWRTMyOutput.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax + Cross Entropy Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropyLayer:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = input_shape\n",
    "        \n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert input.shape == self.input_shape, \"Input shape is not correct.\"\n",
    "        exps = np.exp(input - np.max(input, axis=-1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        self.input = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y_true, learning_rate):\n",
    "        # Gradient of Cross-Entropy loss with respect to the logits (input to softmax)\n",
    "        grad = self.output - y_true\n",
    "        return grad\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-12\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred + epsilon)) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape and normalize data\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Model: CNN, Sigmoid, AveragePool, Flatten, Reshape, CNN, Reshape, Softmax\n",
    "\n",
    "Input Shapes: (28, 28, 1), (26, 26, 2), (26, 26, 2), (13, 13, 2), (338,), (1, 1, 338), (1, 1, 10), (1, 10)\n",
    "\n",
    "Output Shapes: (26, 26, 2), (26, 26, 2), (13, 13, 2), (338,), (1, 1, 338), (1, 1, 10), (1, 10), (1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstCnnLayer = CNNLayer(kernel_size=3, number_of_kernels=2, input_shape=(28, 28, 1))\n",
    "sigmoidLayer = SigmoidLayer(input_shape=(26, 26, 2))\n",
    "averagePoolLayer = AveragePoolLayer(input_shape=(26, 26, 2), pool_size=2, stride=2)\n",
    "flattenLayer = FlattenLayer(input_shape=(13, 13, 2))\n",
    "reshapeLayer1 = ReshapeLayer(input_shape=(338,), output_shape=(1, 1, 338))\n",
    "secondCnnLayer = CNNLayer(kernel_size=1, number_of_kernels=10, input_shape=(1, 1, 338))\n",
    "reshapeLayer2 = ReshapeLayer(input_shape=(1, 1, 10), output_shape=(1, 10))\n",
    "softmaxCrossEntropyLayer = SoftmaxCrossEntropyLayer(input_shape=(1, 10))\n",
    "\n",
    "model = [firstCnnLayer, sigmoidLayer, averagePoolLayer, flattenLayer, reshapeLayer1, secondCnnLayer, reshapeLayer2, softmaxCrossEntropyLayer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, x):\n",
    "    for layer in model:\n",
    "        x = layer.forward(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(model, y_true, learning_rate):\n",
    "    grad = y_true\n",
    "    for layer in reversed(model):\n",
    "        grad = layer.backward(grad, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, epochs, learning_rate, print_every=100):\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        \n",
    "        for i in range(len(x_train)):\n",
    "            # Forward pass\n",
    "            output = forward(model, x_train[i])\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = cross_entropy_loss(y_train[i], output)\n",
    "            total_loss += loss\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if np.argmax(output) == np.argmax(y_train[i]):\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            # Backward pass\n",
    "            backward(model, y_train[i], learning_rate)\n",
    "            \n",
    "            # Print loss and accuracy every 'print_every' iterations\n",
    "            if (i + 1) % print_every == 0:\n",
    "                avg_loss = total_loss / print_every\n",
    "                accuracy = correct_predictions / print_every\n",
    "                print(f\"Iteration {i + 1} in Epoch {epoch + 1}, Loss: {avg_loss}, Accuracy: {accuracy}\")\n",
    "                losses.append(avg_loss)\n",
    "                accuracies.append(accuracy)\n",
    "                total_loss = 0\n",
    "                correct_predictions = 0\n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x_test):\n",
    "    predictions = []\n",
    "    for i in range(len(x_test)):\n",
    "        output = forward(model, x_test[i])\n",
    "        predictions.append(np.argmax(output))\n",
    "    return predictions\n",
    "\n",
    "def evaluate(model, x_test, y_test):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(x_test)):\n",
    "        output = forward(model, x_test[i])\n",
    "        if np.argmax(output) == np.argmax(y_test[i]):\n",
    "            correct_predictions += 1\n",
    "    return correct_predictions / len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100 in Epoch 1, Loss: 0.6819090176419565, Accuracy: 0.11\n",
      "Iteration 200 in Epoch 1, Loss: 0.3334971494822155, Accuracy: 0.1\n",
      "Iteration 300 in Epoch 1, Loss: 0.2918312238084903, Accuracy: 0.1\n",
      "Iteration 400 in Epoch 1, Loss: 0.26490881610702166, Accuracy: 0.13\n",
      "Iteration 500 in Epoch 1, Loss: 0.26186692592829314, Accuracy: 0.09\n",
      "Iteration 600 in Epoch 1, Loss: 0.2482931380893328, Accuracy: 0.13\n",
      "Iteration 700 in Epoch 1, Loss: 0.24603199004859952, Accuracy: 0.16\n",
      "Iteration 800 in Epoch 1, Loss: 0.2563267268628861, Accuracy: 0.08\n",
      "Iteration 900 in Epoch 1, Loss: 0.24593875275466068, Accuracy: 0.16\n",
      "Iteration 1000 in Epoch 1, Loss: 0.2466037646540307, Accuracy: 0.15\n",
      "Iteration 1100 in Epoch 1, Loss: 0.24989736231839918, Accuracy: 0.24\n",
      "Iteration 1200 in Epoch 1, Loss: 0.25000553788646485, Accuracy: 0.16\n",
      "Iteration 1300 in Epoch 1, Loss: 0.23873753556602925, Accuracy: 0.22\n",
      "Iteration 1400 in Epoch 1, Loss: 0.25360437567234073, Accuracy: 0.16\n",
      "Iteration 1500 in Epoch 1, Loss: 0.25638976305893474, Accuracy: 0.11\n",
      "Iteration 1600 in Epoch 1, Loss: 0.24977415699362523, Accuracy: 0.11\n",
      "Iteration 1700 in Epoch 1, Loss: 0.24847151270466086, Accuracy: 0.14\n",
      "Iteration 1800 in Epoch 1, Loss: 0.24082482010680326, Accuracy: 0.17\n",
      "Iteration 1900 in Epoch 1, Loss: 0.25335727942785075, Accuracy: 0.16\n",
      "Iteration 2000 in Epoch 1, Loss: 0.24957178849393297, Accuracy: 0.15\n",
      "Iteration 2100 in Epoch 1, Loss: 0.2420288931747556, Accuracy: 0.18\n",
      "Iteration 2200 in Epoch 1, Loss: 0.23209844819850686, Accuracy: 0.21\n",
      "Iteration 2300 in Epoch 1, Loss: 0.2388037915796777, Accuracy: 0.19\n",
      "Iteration 2400 in Epoch 1, Loss: 0.23936148860622855, Accuracy: 0.17\n",
      "Iteration 2500 in Epoch 1, Loss: 0.2362242659262825, Accuracy: 0.19\n",
      "Iteration 2600 in Epoch 1, Loss: 0.23377712299697248, Accuracy: 0.18\n",
      "Iteration 2700 in Epoch 1, Loss: 0.23134358634609534, Accuracy: 0.2\n",
      "Iteration 2800 in Epoch 1, Loss: 0.2354617352234438, Accuracy: 0.23\n",
      "Iteration 2900 in Epoch 1, Loss: 0.22828613736217374, Accuracy: 0.26\n",
      "Iteration 3000 in Epoch 1, Loss: 0.22999957712769414, Accuracy: 0.24\n",
      "Iteration 3100 in Epoch 1, Loss: 0.2220181184321968, Accuracy: 0.27\n",
      "Iteration 3200 in Epoch 1, Loss: 0.2253322518639198, Accuracy: 0.29\n",
      "Iteration 3300 in Epoch 1, Loss: 0.2161493382249512, Accuracy: 0.31\n",
      "Iteration 3400 in Epoch 1, Loss: 0.23182365911014216, Accuracy: 0.19\n",
      "Iteration 3500 in Epoch 1, Loss: 0.20663587105029915, Accuracy: 0.31\n",
      "Iteration 3600 in Epoch 1, Loss: 0.2305297461227328, Accuracy: 0.26\n",
      "Iteration 3700 in Epoch 1, Loss: 0.23534536824443303, Accuracy: 0.2\n",
      "Iteration 3800 in Epoch 1, Loss: 0.224728429292668, Accuracy: 0.21\n",
      "Iteration 3900 in Epoch 1, Loss: 0.22233662469614118, Accuracy: 0.28\n",
      "Iteration 4000 in Epoch 1, Loss: 0.215799923400091, Accuracy: 0.33\n",
      "Iteration 4100 in Epoch 1, Loss: 0.21156996972576164, Accuracy: 0.32\n",
      "Iteration 4200 in Epoch 1, Loss: 0.2143830857398247, Accuracy: 0.38\n",
      "Iteration 4300 in Epoch 1, Loss: 0.2041873699982051, Accuracy: 0.33\n",
      "Iteration 4400 in Epoch 1, Loss: 0.21830977516498223, Accuracy: 0.35\n",
      "Iteration 4500 in Epoch 1, Loss: 0.2273616102235759, Accuracy: 0.26\n",
      "Iteration 4600 in Epoch 1, Loss: 0.18538641898352692, Accuracy: 0.35\n",
      "Iteration 4700 in Epoch 1, Loss: 0.21336306005145406, Accuracy: 0.26\n",
      "Iteration 4800 in Epoch 1, Loss: 0.20031947408769962, Accuracy: 0.31\n",
      "Iteration 4900 in Epoch 1, Loss: 0.2038615157934856, Accuracy: 0.36\n",
      "Iteration 5000 in Epoch 1, Loss: 0.19414571509816106, Accuracy: 0.35\n",
      "Iteration 5100 in Epoch 1, Loss: 0.1945766988586399, Accuracy: 0.35\n",
      "Iteration 5200 in Epoch 1, Loss: 0.18992480029467376, Accuracy: 0.37\n",
      "Iteration 5300 in Epoch 1, Loss: 0.19136528610610695, Accuracy: 0.42\n",
      "Iteration 5400 in Epoch 1, Loss: 0.18320073718021995, Accuracy: 0.42\n",
      "Iteration 5500 in Epoch 1, Loss: 0.16347547754682157, Accuracy: 0.42\n",
      "Iteration 5600 in Epoch 1, Loss: 0.15515774116136188, Accuracy: 0.49\n",
      "Iteration 5700 in Epoch 1, Loss: 0.17536890133579225, Accuracy: 0.42\n",
      "Iteration 5800 in Epoch 1, Loss: 0.19266327810355535, Accuracy: 0.37\n",
      "Iteration 5900 in Epoch 1, Loss: 0.16398303201941897, Accuracy: 0.4\n",
      "Iteration 6000 in Epoch 1, Loss: 0.1589680395984153, Accuracy: 0.45\n",
      "Iteration 6100 in Epoch 1, Loss: 0.14468347266732556, Accuracy: 0.52\n",
      "Iteration 6200 in Epoch 1, Loss: 0.15371769367768637, Accuracy: 0.48\n",
      "Iteration 6300 in Epoch 1, Loss: 0.16698602553562503, Accuracy: 0.42\n",
      "Iteration 6400 in Epoch 1, Loss: 0.15170327881843013, Accuracy: 0.52\n",
      "Iteration 6500 in Epoch 1, Loss: 0.1587528075894603, Accuracy: 0.41\n",
      "Iteration 6600 in Epoch 1, Loss: 0.13228536858282325, Accuracy: 0.6\n",
      "Iteration 6700 in Epoch 1, Loss: 0.14561511877015618, Accuracy: 0.49\n",
      "Iteration 6800 in Epoch 1, Loss: 0.13943423174431047, Accuracy: 0.48\n",
      "Iteration 6900 in Epoch 1, Loss: 0.1825271731991299, Accuracy: 0.39\n",
      "Iteration 7000 in Epoch 1, Loss: 0.1257009979838087, Accuracy: 0.56\n",
      "Iteration 7100 in Epoch 1, Loss: 0.15310964074071864, Accuracy: 0.51\n",
      "Iteration 7200 in Epoch 1, Loss: 0.14594310654277234, Accuracy: 0.49\n",
      "Iteration 7300 in Epoch 1, Loss: 0.16525249996543956, Accuracy: 0.44\n",
      "Iteration 7400 in Epoch 1, Loss: 0.1327175114437023, Accuracy: 0.51\n",
      "Iteration 7500 in Epoch 1, Loss: 0.13821887003895872, Accuracy: 0.52\n",
      "Iteration 7600 in Epoch 1, Loss: 0.12730647803399456, Accuracy: 0.59\n",
      "Iteration 7700 in Epoch 1, Loss: 0.13847544766752928, Accuracy: 0.55\n",
      "Iteration 7800 in Epoch 1, Loss: 0.16402072138665716, Accuracy: 0.45\n",
      "Iteration 7900 in Epoch 1, Loss: 0.14664694870302977, Accuracy: 0.51\n",
      "Iteration 8000 in Epoch 1, Loss: 0.13430739616104787, Accuracy: 0.56\n",
      "Iteration 8100 in Epoch 1, Loss: 0.1080836107589048, Accuracy: 0.72\n",
      "Iteration 8200 in Epoch 1, Loss: 0.12324137884236983, Accuracy: 0.59\n",
      "Iteration 8300 in Epoch 1, Loss: 0.15796713050160205, Accuracy: 0.45\n",
      "Iteration 8400 in Epoch 1, Loss: 0.13491300884733304, Accuracy: 0.56\n",
      "Iteration 8500 in Epoch 1, Loss: 0.13835069974760023, Accuracy: 0.51\n",
      "Iteration 8600 in Epoch 1, Loss: 0.09808040100187704, Accuracy: 0.69\n",
      "Iteration 8700 in Epoch 1, Loss: 0.13737432500769223, Accuracy: 0.57\n",
      "Iteration 8800 in Epoch 1, Loss: 0.15131204394209805, Accuracy: 0.54\n",
      "Iteration 8900 in Epoch 1, Loss: 0.13953911730673987, Accuracy: 0.52\n",
      "Iteration 9000 in Epoch 1, Loss: 0.10238423792094295, Accuracy: 0.69\n",
      "Iteration 9100 in Epoch 1, Loss: 0.09988170196826063, Accuracy: 0.67\n",
      "Iteration 9200 in Epoch 1, Loss: 0.10278723531029767, Accuracy: 0.66\n",
      "Iteration 9300 in Epoch 1, Loss: 0.11487517714606583, Accuracy: 0.64\n",
      "Iteration 9400 in Epoch 1, Loss: 0.09910619074492617, Accuracy: 0.7\n",
      "Iteration 9500 in Epoch 1, Loss: 0.11161281446181222, Accuracy: 0.6\n",
      "Iteration 9600 in Epoch 1, Loss: 0.1352668082633939, Accuracy: 0.5\n",
      "Iteration 9700 in Epoch 1, Loss: 0.09478669790700726, Accuracy: 0.72\n",
      "Iteration 9800 in Epoch 1, Loss: 0.0912807178502852, Accuracy: 0.69\n",
      "Iteration 9900 in Epoch 1, Loss: 0.09100920793895673, Accuracy: 0.75\n",
      "Iteration 10000 in Epoch 1, Loss: 0.07882883887014586, Accuracy: 0.73\n",
      "Iteration 10100 in Epoch 1, Loss: 0.09846467732037423, Accuracy: 0.71\n",
      "Iteration 10200 in Epoch 1, Loss: 0.0969291566860913, Accuracy: 0.66\n",
      "Iteration 10300 in Epoch 1, Loss: 0.12423556421110256, Accuracy: 0.63\n",
      "Iteration 10400 in Epoch 1, Loss: 0.08830344447043716, Accuracy: 0.74\n",
      "Iteration 10500 in Epoch 1, Loss: 0.09154675832496079, Accuracy: 0.71\n",
      "Iteration 10600 in Epoch 1, Loss: 0.07728804516935343, Accuracy: 0.72\n",
      "Iteration 10700 in Epoch 1, Loss: 0.07551933164113538, Accuracy: 0.7\n",
      "Iteration 10800 in Epoch 1, Loss: 0.08857499808058769, Accuracy: 0.7\n",
      "Iteration 10900 in Epoch 1, Loss: 0.07095062072282012, Accuracy: 0.82\n",
      "Iteration 11000 in Epoch 1, Loss: 0.09307729908488035, Accuracy: 0.67\n",
      "Iteration 11100 in Epoch 1, Loss: 0.0784017941314051, Accuracy: 0.78\n",
      "Iteration 11200 in Epoch 1, Loss: 0.09862487103288296, Accuracy: 0.65\n",
      "Iteration 11300 in Epoch 1, Loss: 0.09816912193361603, Accuracy: 0.68\n",
      "Iteration 11400 in Epoch 1, Loss: 0.087462921291172, Accuracy: 0.72\n",
      "Iteration 11500 in Epoch 1, Loss: 0.09170570347422563, Accuracy: 0.73\n",
      "Iteration 11600 in Epoch 1, Loss: 0.10460446883904542, Accuracy: 0.7\n",
      "Iteration 11700 in Epoch 1, Loss: 0.11120685488335896, Accuracy: 0.63\n",
      "Iteration 11800 in Epoch 1, Loss: 0.11064066366300727, Accuracy: 0.68\n",
      "Iteration 11900 in Epoch 1, Loss: 0.07280274324209099, Accuracy: 0.74\n",
      "Iteration 12000 in Epoch 1, Loss: 0.08661589796909047, Accuracy: 0.74\n",
      "Iteration 12100 in Epoch 1, Loss: 0.10175906563389826, Accuracy: 0.7\n",
      "Iteration 12200 in Epoch 1, Loss: 0.10253736607785044, Accuracy: 0.69\n",
      "Iteration 12300 in Epoch 1, Loss: 0.09605157863705101, Accuracy: 0.67\n",
      "Iteration 12400 in Epoch 1, Loss: 0.09827614802750909, Accuracy: 0.66\n",
      "Iteration 12500 in Epoch 1, Loss: 0.09330770093444242, Accuracy: 0.72\n",
      "Iteration 12600 in Epoch 1, Loss: 0.1172950464130769, Accuracy: 0.6\n",
      "Iteration 12700 in Epoch 1, Loss: 0.1249230534887293, Accuracy: 0.63\n",
      "Iteration 12800 in Epoch 1, Loss: 0.0871707722164673, Accuracy: 0.7\n",
      "Iteration 12900 in Epoch 1, Loss: 0.08419475328355366, Accuracy: 0.69\n",
      "Iteration 13000 in Epoch 1, Loss: 0.10878764448758148, Accuracy: 0.67\n",
      "Iteration 13100 in Epoch 1, Loss: 0.11706613302265029, Accuracy: 0.63\n",
      "Iteration 13200 in Epoch 1, Loss: 0.1134289046650213, Accuracy: 0.63\n",
      "Iteration 13300 in Epoch 1, Loss: 0.08389270192740517, Accuracy: 0.69\n",
      "Iteration 13400 in Epoch 1, Loss: 0.09464728897895848, Accuracy: 0.73\n",
      "Iteration 13500 in Epoch 1, Loss: 0.05982191382087224, Accuracy: 0.83\n",
      "Iteration 13600 in Epoch 1, Loss: 0.061669202555758515, Accuracy: 0.77\n",
      "Iteration 13700 in Epoch 1, Loss: 0.09947564209553965, Accuracy: 0.73\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m losses, accuracies \u001b[39m=\u001b[39m train(model, x_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m)\n",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     correct_predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m backward(model, y_train[i], learning_rate)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Print loss and accuracy every 'print_every' iterations\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mif\u001b[39;00m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m print_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb Cell 28\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m grad \u001b[39m=\u001b[39m y_true\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(model):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     grad \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(grad, learning_rate)\n",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb Cell 28\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m         \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(kernel\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m             \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(kernel\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m                 \u001b[39mif\u001b[39;00m x \u001b[39m-\u001b[39m k \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m x \u001b[39m-\u001b[39m k \u001b[39m<\u001b[39m channel\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mand\u001b[39;00m y \u001b[39m-\u001b[39m l \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m y \u001b[39m-\u001b[39;49m l \u001b[39m<\u001b[39m channel\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m                     result[x, y] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m channel[x \u001b[39m-\u001b[39m k, y \u001b[39m-\u001b[39m l] \u001b[39m*\u001b[39m kernel[k, l]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m input_grads[:, :, j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m result                \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, accuracies = train(model, x_train, y_train, epochs=5, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Training Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(losses)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mTraining Duration\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# Doesn't really make sense, but it is just for visualization purposes\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.224.33.107/home/nyuad/Documents/GitHub/AML_Projects/Bonus_Project/Bonus_Project.ipynb#X52sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m\"\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel(\"Training Duration\") # Doesn't really make sense, but it is just for visualization purposes\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accuracies)\n",
    "plt.xlabel(\"Training Duration\") # Doesn't really make sense, but it is just for visualization purposes\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: \", evaluate(model, x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bonus-project-vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
